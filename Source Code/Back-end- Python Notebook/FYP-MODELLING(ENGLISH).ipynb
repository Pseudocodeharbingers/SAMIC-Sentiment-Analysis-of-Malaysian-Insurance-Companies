{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "import re, string\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "from IPython.display import IFrame\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster, FastMarkerCluster, HeatMapWithTime\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1)\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212542</th>\n",
       "      <td>4</td>\n",
       "      <td>dont ignore me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212543</th>\n",
       "      <td>4</td>\n",
       "      <td>I'm comin wit u!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212544</th>\n",
       "      <td>4</td>\n",
       "      <td>I want to go to VP, but no one is willing to come with me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212545</th>\n",
       "      <td>4</td>\n",
       "      <td>Wah, why are you sad?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212546</th>\n",
       "      <td>4</td>\n",
       "      <td>playing sudoku while mommy makes me breakfast &amp;amp; lunch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1212547 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "0        0       \n",
       "1        0       \n",
       "2        0       \n",
       "3        0       \n",
       "4        0       \n",
       "...     ..       \n",
       "1212542  4       \n",
       "1212543  4       \n",
       "1212544  4       \n",
       "1212545  4       \n",
       "1212546  4       \n",
       "\n",
       "                                                                                                                       tweet  \n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      \n",
       "2        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            \n",
       "3        my whole body feels itchy and like its on fire                                                                       \n",
       "4        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       \n",
       "...                                                                                                                  ...      \n",
       "1212542   dont ignore me                                                                                                      \n",
       "1212543   I'm comin wit u!!                                                                                                   \n",
       "1212544  I want to go to VP, but no one is willing to come with me.                                                           \n",
       "1212545   Wah, why are you sad?                                                                                               \n",
       "1212546  playing sudoku while mommy makes me breakfast &amp; lunch                                                            \n",
       "\n",
       "[1212547 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label 0-negative, 2-means positive, 4-neutral\n",
    "data = pd.read_csv(\"trainingdata_english.csv\",skip_blank_lines=True,encoding = \"latin\") \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative tagged sentences is: 600000\n",
      "number of positive tagged sentences is: 600000\n",
      "number of neutral tagged sentences  is: 12547\n",
      "total length of the data is:            1212547\n"
     ]
    }
   ],
   "source": [
    "# check the number of positive vs. negative tagged sentences\n",
    "negatives = data['label'][data.label == 0]\n",
    "positives = data['label'][data.label == 2]\n",
    "neutral = data['label'][data.label == 4]\n",
    "\n",
    "print('number of negative tagged sentences is: {}'.format(len(negatives)))\n",
    "print('number of positive tagged sentences is: {}'.format(len(positives)))\n",
    "print('number of neutral tagged sentences  is: {}'.format(len(neutral)))\n",
    "print('total length of the data is:            {}'.format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing 1:  Clean tweet text by removing links, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(tweet):\n",
    "    tweet = tweet.lower() # convert text to lower-case\n",
    "    tweet = re.sub(r'\\&\\w*;', '',tweet) # remove HTML special entities (e.g. &amp;)\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet) # remove tickers\n",
    "    tweet = re.sub(r'#\\w*', '', tweet) # remove hashtags\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet) # remove hyperlinks\n",
    "    tweet = tweet.lstrip(' ') # remove single space remaining at the front of the tweet.\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet) # remove whitespace (including new line characters) \n",
    "    tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet) # remove punctuation and split 's, 't, 've with a space for filter\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet) # remove words with 2 or fewer letters \n",
    "    tweet = tweet.lstrip(' ') # remove single space remaining at the front of the tweet.\n",
    "  \n",
    "    # remove characters beyond Basic Multilingual Plane (BMP) of unicode (contains characters for almost all modern languages, and a large number of symbols):\n",
    "    tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n",
    "    return tweet\n",
    "\n",
    "    # clean dataframe's text column\n",
    "    data['tweet'] = data['tweet'].apply(text_cleaning)\n",
    "    # preview some cleaned tweets\n",
    "    data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    int64 \n",
      "tweet    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@Born_4_Broadway Lost  and it was St. Ignacius Prepatory School. Haha.',\n",
       " '\\n',\n",
       " 'USER lost and  was   ignacius prepatory school  haha ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = data['tweet'][600]\n",
    "after = text_cleaning(before)\n",
    "\n",
    "before,'\\n',after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198985, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates\n",
    "data = data.drop_duplicates('tweet')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pre-processing 2: Tokenize without out Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nanan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"i , me , my , myself , we , our , ours , ourselves , you , you're , you've , you'll , you'd , your , yours , yourself , yourselves , he , him , his , himself , she , she's , her , hers , herself , it \",\n",
       " 179)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show stop words examples\n",
    "stop_words = stopwords.words('english') # show some stop words\n",
    "' , '.join(stop_words)[:200], len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nanan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nanan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>[switchfoot, httptwitpiccom2y1zl, awww, thats, bummer, shoulda, got, david, carr, third, day]</td>\n",
       "      <td>[switchfoot, httptwitpiccom2y1zl, awww, thats, bummer, shoulda, got, david, carr, third, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might, cry, result, school, today, also, blah]</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might, cry, result, school, today, also, blah]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "      <td>[kenichan, dived, many, times, ball, managed, save, 50, rest, go, bounds]</td>\n",
       "      <td>[kenichan, dived, many, times, ball, managed, save, 50, rest, go, bounds]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "      <td>[nationwideclass, behaving, im, mad, cant, see]</td>\n",
       "      <td>[nationwideclass, behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0  0       \n",
       "1  0       \n",
       "2  0       \n",
       "3  0       \n",
       "4  0       \n",
       "\n",
       "                                                                                                                 tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D   \n",
       "1  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!       \n",
       "2  @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                             \n",
       "3  my whole body feels itchy and like its on fire                                                                        \n",
       "4  @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.        \n",
       "\n",
       "                                                                                          tokens  \\\n",
       "0  [switchfoot, httptwitpiccom2y1zl, awww, thats, bummer, shoulda, got, david, carr, third, day]   \n",
       "1  [upset, cant, update, facebook, texting, might, cry, result, school, today, also, blah]         \n",
       "2  [kenichan, dived, many, times, ball, managed, save, 50, rest, go, bounds]                       \n",
       "3  [whole, body, feels, itchy, like, fire]                                                         \n",
       "4  [nationwideclass, behaving, im, mad, cant, see]                                                 \n",
       "\n",
       "                                                                                           words  \n",
       "0  [switchfoot, httptwitpiccom2y1zl, awww, thats, bummer, shoulda, got, david, carr, third, day]  \n",
       "1  [upset, cant, update, facebook, texting, might, cry, result, school, today, also, blah]        \n",
       "2  [kenichan, dived, many, times, ball, managed, save, 50, rest, go, bounds]                      \n",
       "3  [whole, body, feels, itchy, like, fire]                                                        \n",
       "4  [nationwideclass, behaving, im, mad, cant, see]                                                "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize helper function\n",
    "def process_text(text):\n",
    "    nopunc = [char for char in list(text) if char not in string.punctuation] # check characters to see if they are in punctuation\n",
    "    nopunc = ''.join(nopunc) # join the characters again to form the string\n",
    "    return [word for word in nopunc.lower().split() if word.lower() not in stopwords.words('english')] # remove any stopwords\n",
    "\n",
    "def remove_words(word_list):\n",
    "    remove = ['com','pic','twitter','...','“','”','’','…']\n",
    "    return [w for w in word_list if w not in remove]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "# tokenize message column and create a column for tokens\n",
    "data = data.copy()\n",
    "data['tokens'] = data['tweet'].apply(process_text)\n",
    "data['words'] = data['tokens'].apply(remove_words)\n",
    "data['lemmatize'] = data['tweet'].apply(lemmatize_tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [switchfoot, httptwitpiccom2y1zl, awww, thats, bummer, shoulda, got, david, carr, third, day]\n",
       "1    [upset, cant, update, facebook, texting, might, cry, result, school, today, also, blah]      \n",
       "2    [kenichan, dived, many, times, ball, managed, save, 50, rest, go, bounds]                    \n",
       "3    [whole, body, feels, itchy, like, fire]                                                      \n",
       "4    [nationwideclass, behaving, im, mad, cant, see]                                              \n",
       "5    [kwesidei, whole, crew]                                                                      \n",
       "6    [need, hug]                                                                                  \n",
       "7    [loltrish, hey, long, time, see, yes, rains, bit, bit, lol, im, fine, thanks, hows]          \n",
       "8    [tatianak, nope, didnt]                                                                      \n",
       "9    [twittera, que, muera]                                                                       \n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# style 1 tokens\n",
    "data['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pre-processing 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorize (convert each message which is represented by a list of tokens into a vector that a machine learning model can understand)\n",
    "# converts a collection of text documents to a matrix of token counts\n",
    "bow_transformer = CountVectorizer(analyzer=process_text).fit(data['tweet']) \n",
    "# print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'switchfoot': 586774,\n",
       " 'httptwitpiccom2y1zl': 275231,\n",
       " 'awww': 53262,\n",
       " 'thats': 598044,\n",
       " 'bummer': 88902,\n",
       " 'shoulda': 550162,\n",
       " 'got': 217465,\n",
       " 'david': 138818,\n",
       " 'carr': 97598,\n",
       " 'third': 604607,\n",
       " 'day': 139703,\n",
       " 'upset': 634437,\n",
       " 'cant': 95197,\n",
       " 'update': 633892,\n",
       " 'facebook': 183836,\n",
       " 'texting': 596539,\n",
       " 'might': 407154,\n",
       " 'cry': 129356,\n",
       " 'result': 515240,\n",
       " 'school': 536560,\n",
       " 'today': 611884,\n",
       " 'also': 30732,\n",
       " 'blah': 73151,\n",
       " 'kenichan': 344071,\n",
       " 'dived': 152443,\n",
       " 'many': 390250,\n",
       " 'times': 608794,\n",
       " 'ball': 57514,\n",
       " 'managed': 388838,\n",
       " 'save': 534662,\n",
       " '50': 9889,\n",
       " 'rest': 515003,\n",
       " 'go': 214361,\n",
       " 'bounds': 80142,\n",
       " 'whole': 654391,\n",
       " 'body': 76901,\n",
       " 'feels': 188484,\n",
       " 'itchy': 309385,\n",
       " 'like': 368375,\n",
       " 'fire': 191807,\n",
       " 'nationwideclass': 434002,\n",
       " 'behaving': 64632,\n",
       " 'im': 300985,\n",
       " 'mad': 385133,\n",
       " 'see': 540001,\n",
       " 'kwesidei': 355371,\n",
       " 'crew': 127957,\n",
       " 'need': 435435,\n",
       " 'hug': 293413,\n",
       " 'loltrish': 375452,\n",
       " 'hey': 236184,\n",
       " 'long': 375888,\n",
       " 'time': 608467,\n",
       " 'yes': 672532,\n",
       " 'rains': 505304,\n",
       " 'bit': 71697,\n",
       " 'lol': 374707,\n",
       " 'fine': 191302,\n",
       " 'thanks': 597319,\n",
       " 'hows': 244785,\n",
       " 'tatianak': 591499,\n",
       " 'nope': 445474,\n",
       " 'didnt': 149048,\n",
       " 'twittera': 626585,\n",
       " 'que': 492001,\n",
       " 'muera': 426220,\n",
       " 'spring': 569498,\n",
       " 'break': 82369,\n",
       " 'plain': 477710,\n",
       " 'city': 112917,\n",
       " 'snowing': 561474,\n",
       " 'repierced': 514062,\n",
       " 'ears': 165391,\n",
       " 'caregiving': 96324,\n",
       " 'couldnt': 124747,\n",
       " 'bear': 62028,\n",
       " 'watch': 647174,\n",
       " 'thought': 605897,\n",
       " 'ua': 629122,\n",
       " 'loss': 377383,\n",
       " 'embarrassing': 171817,\n",
       " 'octolinz16': 450857,\n",
       " 'counts': 125016,\n",
       " 'idk': 298434,\n",
       " 'either': 168968,\n",
       " 'never': 437476,\n",
       " 'talk': 589312,\n",
       " 'anymore': 41228,\n",
       " 'smarrison': 559269,\n",
       " 'wouldve': 661986,\n",
       " 'first': 192104,\n",
       " 'gun': 223160,\n",
       " 'really': 509061,\n",
       " 'though': 605714,\n",
       " 'zac': 677143,\n",
       " 'snyders': 561655,\n",
       " 'doucheclown': 158183,\n",
       " 'iamjazzyfizzle': 296424,\n",
       " 'wish': 657506,\n",
       " 'miss': 411195,\n",
       " 'iamlilnicki': 296534,\n",
       " 'premiere': 484506,\n",
       " 'hollis': 240603,\n",
       " 'death': 142165,\n",
       " 'scene': 536063,\n",
       " 'hurt': 294747,\n",
       " 'severely': 542866,\n",
       " 'film': 190726,\n",
       " 'wry': 663139,\n",
       " 'directors': 150858,\n",
       " 'cut': 131418,\n",
       " 'file': 190594,\n",
       " 'taxes': 591838,\n",
       " 'lettya': 366098,\n",
       " 'ahh': 23091,\n",
       " 'ive': 311209,\n",
       " 'always': 31110,\n",
       " 'wanted': 646276,\n",
       " 'rent': 513835,\n",
       " 'love': 378213,\n",
       " 'soundtrack': 566133,\n",
       " 'fakerpattypattz': 184972,\n",
       " 'oh': 452179,\n",
       " 'dear': 142025,\n",
       " 'drinking': 160653,\n",
       " 'forgotten': 197265,\n",
       " 'table': 588260,\n",
       " 'drinks': 160701,\n",
       " 'alydesigns': 31294,\n",
       " 'get': 209892,\n",
       " 'much': 425889,\n",
       " 'done': 156437,\n",
       " 'one': 455681,\n",
       " 'friend': 200631,\n",
       " 'called': 93310,\n",
       " 'asked': 47803,\n",
       " 'meet': 399992,\n",
       " 'mid': 406731,\n",
       " 'valley': 637081,\n",
       " 'todaybut': 611987,\n",
       " 'sigh': 552302,\n",
       " 'angrybarista': 38127,\n",
       " 'baked': 57235,\n",
       " 'cake': 92721,\n",
       " 'ated': 49136,\n",
       " 'week': 649624,\n",
       " 'going': 215121,\n",
       " 'hoped': 242710,\n",
       " 'blagh': 73141,\n",
       " 'class': 113989,\n",
       " '8': 12527,\n",
       " 'tomorrow': 613891,\n",
       " 'hate': 230015,\n",
       " 'call': 93260,\n",
       " 'wake': 645367,\n",
       " 'people': 470427,\n",
       " 'sleep': 557443,\n",
       " 'watching': 647236,\n",
       " 'marley': 392866,\n",
       " 'sad': 528526,\n",
       " 'misslilly': 412169,\n",
       " 'ooooh': 457286,\n",
       " 'leslie': 365601,\n",
       " 'ok': 453394,\n",
       " 'wont': 659402,\n",
       " 'meh': 400938,\n",
       " 'almost': 29955,\n",
       " 'lover': 378914,\n",
       " 'exception': 181415,\n",
       " 'track': 617926,\n",
       " 'gets': 210079,\n",
       " 'depressed': 145870,\n",
       " 'every': 179955,\n",
       " 'some1': 563165,\n",
       " 'hacked': 225013,\n",
       " 'account': 17273,\n",
       " 'aim': 23965,\n",
       " 'make': 387543,\n",
       " 'new': 437636,\n",
       " 'alielayus': 27976,\n",
       " 'want': 646252,\n",
       " 'promote': 487619,\n",
       " 'gear': 207836,\n",
       " 'groove': 221130,\n",
       " 'unfornately': 632341,\n",
       " 'ride': 517852,\n",
       " 'may': 396655,\n",
       " 'b': 54396,\n",
       " 'anaheim': 35242,\n",
       " 'sleeping': 557608,\n",
       " 'option': 458223,\n",
       " 'realizing': 508962,\n",
       " 'evaluations': 179227,\n",
       " 'morning': 420063,\n",
       " 'work': 660408,\n",
       " 'afternoon': 21296,\n",
       " 'julieebaby': 334009,\n",
       " 'awe': 52418,\n",
       " '1': 625,\n",
       " 'humpninja': 294095,\n",
       " 'asian': 47629,\n",
       " 'eyes': 183108,\n",
       " 'night': 440742,\n",
       " 'sick': 551600,\n",
       " 'spent': 568062,\n",
       " 'hour': 243996,\n",
       " 'sitting': 555250,\n",
       " 'shower': 550391,\n",
       " 'cause': 99693,\n",
       " 'stand': 571685,\n",
       " 'held': 233761,\n",
       " 'back': 55680,\n",
       " 'puke': 489496,\n",
       " 'champ': 102813,\n",
       " 'bed': 63343,\n",
       " 'cocomix04': 117016,\n",
       " 'ill': 300200,\n",
       " 'tell': 594679,\n",
       " 'ya': 669517,\n",
       " 'story': 576807,\n",
       " 'later': 360073,\n",
       " 'good': 215852,\n",
       " 'workin': 660697,\n",
       " 'three': 606148,\n",
       " 'hours': 244058,\n",
       " 'missxu': 412970,\n",
       " 'sorry': 565411,\n",
       " 'came': 93851,\n",
       " 'gmt1': 214154,\n",
       " 'httpisgdfnge': 259347,\n",
       " 'fleurylis': 193623,\n",
       " 'dont': 156958,\n",
       " 'depressing': 145904,\n",
       " 'think': 604308,\n",
       " 'even': 179509,\n",
       " 'know': 351115,\n",
       " 'kids': 346663,\n",
       " 'suitcases': 580835,\n",
       " '812': 12682,\n",
       " '123': 1923,\n",
       " 'gym': 224208,\n",
       " '35': 7679,\n",
       " '6': 10915,\n",
       " '610': 11032,\n",
       " 'another': 39996,\n",
       " 'gonna': 215750,\n",
       " 'fly': 194829,\n",
       " 'girlfriend': 212133,\n",
       " 'feel': 188360,\n",
       " 'getting': 210130,\n",
       " 'study': 578474,\n",
       " 'tomorrows': 614109,\n",
       " 'practical': 483598,\n",
       " 'exam': 181110,\n",
       " 'hes': 235987,\n",
       " 'reason': 509430,\n",
       " 'teardrops': 593265,\n",
       " 'guitar': 222893,\n",
       " 'enough': 174866,\n",
       " 'heart': 232092,\n",
       " 'feeling': 188394,\n",
       " 'wanna': 646161,\n",
       " 'still': 575569,\n",
       " 'jonathanrknight': 329562,\n",
       " 'soo': 564340,\n",
       " 'finally': 191036,\n",
       " 'comfortable': 119249,\n",
       " 'missed': 411635,\n",
       " 'falling': 185151,\n",
       " 'asleep': 47932,\n",
       " 'heard': 232025,\n",
       " 'tracy': 618031,\n",
       " 'girls': 212357,\n",
       " 'found': 197883,\n",
       " 'breaks': 82592,\n",
       " 'family': 185371,\n",
       " 'viennah': 640802,\n",
       " 'yay': 670694,\n",
       " 'happy': 228561,\n",
       " 'job': 326706,\n",
       " 'means': 398996,\n",
       " 'less': 365706,\n",
       " 'checked': 105007,\n",
       " 'user': 635453,\n",
       " 'timeline': 608699,\n",
       " 'blackberry': 72688,\n",
       " 'looks': 376370,\n",
       " 'twanking': 624047,\n",
       " 'happening': 228404,\n",
       " 'ppl': 483389,\n",
       " 'probs': 486749,\n",
       " 'w': 644210,\n",
       " 'bgs': 68857,\n",
       " 'uids': 630366,\n",
       " 'manwas': 390219,\n",
       " 'ironing': 307834,\n",
       " 'jeancjumbes': 318950,\n",
       " 'fave': 187314,\n",
       " 'top': 616014,\n",
       " 'wear': 648511,\n",
       " 'meeting': 400022,\n",
       " 'burnt': 89640,\n",
       " 'strangely': 577127,\n",
       " 'lilo': 369179,\n",
       " 'samro': 531204,\n",
       " 'breaking': 82528,\n",
       " 'tea': 592808,\n",
       " 'retweeting': 515650,\n",
       " 'broadband': 85112,\n",
       " 'plan': 477748,\n",
       " 'massive': 394454,\n",
       " 'broken': 85389,\n",
       " 'promise': 487528,\n",
       " 'httptinyurlcomdcuc33': 269645,\n",
       " 'via': 639817,\n",
       " 'wwwdiigocomtautao': 664337,\n",
       " 'waiting': 645162,\n",
       " 'localtweeps': 374216,\n",
       " 'wow': 662038,\n",
       " 'tons': 615085,\n",
       " 'replies': 514138,\n",
       " 'unfollow': 632298,\n",
       " 'friends': 200772,\n",
       " 'tweets': 624940,\n",
       " 'youre': 675204,\n",
       " 'scrolling': 538407,\n",
       " 'feed': 188228,\n",
       " 'lot': 377585,\n",
       " 'duck': 162457,\n",
       " 'chicken': 107180,\n",
       " 'taking': 589064,\n",
       " 'wayyy': 648083,\n",
       " 'hatch': 229994,\n",
       " 'put': 490608,\n",
       " 'vacation': 636520,\n",
       " 'photos': 474137,\n",
       " 'online': 456425,\n",
       " 'yrs': 675952,\n",
       " 'ago': 22422,\n",
       " 'pc': 468564,\n",
       " 'crashed': 126833,\n",
       " 'forget': 197162,\n",
       " 'name': 432016,\n",
       " 'site': 555099,\n",
       " 'andywana': 37082,\n",
       " 'sure': 583876,\n",
       " 'pos': 482164,\n",
       " 'trade': 618127,\n",
       " 'away': 52276,\n",
       " 'company': 119845,\n",
       " 'assets': 48336,\n",
       " 'andy': 36733,\n",
       " 'oanhlove': 449980,\n",
       " 'happens': 228435,\n",
       " 'dallas': 134125,\n",
       " 'show': 550306,\n",
       " 'gotta': 217636,\n",
       " 'say': 534966,\n",
       " 'youd': 674577,\n",
       " 'shows': 550635,\n",
       " 'would': 661939,\n",
       " 'use': 635388,\n",
       " 'music': 427416,\n",
       " 'game': 205700,\n",
       " 'mmm': 414963,\n",
       " 'ugh92': 629744,\n",
       " 'degrees': 143919,\n",
       " 'u': 629070,\n",
       " 'move': 421652,\n",
       " 'already': 30440,\n",
       " 'sd': 538653,\n",
       " 'hmmm': 239416,\n",
       " 'random': 506148,\n",
       " 'glad': 212993,\n",
       " 'hear': 232017,\n",
       " 'yer': 672490,\n",
       " 'well': 650814,\n",
       " 'batmanyng': 60156,\n",
       " 'ps3': 488431,\n",
       " 'commission': 119614,\n",
       " 'wutcha': 663752,\n",
       " 'playing': 478367,\n",
       " 'copped': 123325,\n",
       " 'blood': 75020,\n",
       " 'sand': 531500,\n",
       " 'leaving': 363378,\n",
       " 'parking': 465616,\n",
       " 'life': 367605,\n",
       " 'cool': 122758,\n",
       " 'sadly': 528794,\n",
       " 'gotten': 217670,\n",
       " 'experience': 182337,\n",
       " 'post': 482436,\n",
       " 'coitus': 117739,\n",
       " 'cigarette': 112070,\n",
       " 'nice': 439045,\n",
       " 'bad': 56238,\n",
       " 'rain': 504939,\n",
       " 'comes': 119178,\n",
       " '5am': 10591,\n",
       " 'starrbby': 572220,\n",
       " 'around': 45060,\n",
       " 'lost': 377407,\n",
       " 'pay': 468281,\n",
       " 'phone': 473738,\n",
       " 'bill': 70433,\n",
       " 'lmao': 373643,\n",
       " 'aw': 51974,\n",
       " 'shucks': 550988,\n",
       " 'damm': 134397,\n",
       " 'mo': 415528,\n",
       " 'jobs': 326854,\n",
       " 'money': 417757,\n",
       " 'hell': 233974,\n",
       " 'min': 409358,\n",
       " 'wage': 644834,\n",
       " '4': 8633,\n",
       " 'fn': 195200,\n",
       " 'clams': 113639,\n",
       " 'katortiz': 340736,\n",
       " 'forever': 197015,\n",
       " 'soon': 564403,\n",
       " 'ltalgonquin': 380092,\n",
       " 'agreed': 22618,\n",
       " 'saw': 534850,\n",
       " 'failwhale': 184597,\n",
       " 'allllll': 29346,\n",
       " 'jdarter': 318580,\n",
       " 'haha': 225315,\n",
       " 'dude': 162573,\n",
       " 'look': 376241,\n",
       " 'em': 171563,\n",
       " 'unless': 632934,\n",
       " 'someone': 563308,\n",
       " 'says': 535189,\n",
       " 'added': 19009,\n",
       " 'terrible': 595772,\n",
       " 'pop': 481408,\n",
       " 'ninjen': 442525,\n",
       " 'right': 518092,\n",
       " 'start': 572375,\n",
       " 'working': 660704,\n",
       " 'nikster': 442056,\n",
       " 'jared': 315835,\n",
       " 'least': 363239,\n",
       " 'diss': 151934,\n",
       " 'bands': 58211,\n",
       " 'trace': 617797,\n",
       " 'clearly': 114632,\n",
       " 'ugly': 629999,\n",
       " 'attire': 49919,\n",
       " 'puma': 489639,\n",
       " 'singlet': 554224,\n",
       " 'adidas': 19441,\n",
       " 'shortsand': 549970,\n",
       " 'black': 72641,\n",
       " 'business': 89899,\n",
       " 'socks': 562185,\n",
       " 'leather': 363273,\n",
       " 'shoes': 549228,\n",
       " 'lucky': 381156,\n",
       " 'run': 526092,\n",
       " 'cute': 131433,\n",
       " 'location': 374230,\n",
       " 'httptwitpiccom2y2es': 275232,\n",
       " 'picnic': 474903,\n",
       " 'smells': 559576,\n",
       " 'citrus': 112903,\n",
       " 'ashleyac': 46916,\n",
       " 'donkey': 156673,\n",
       " 'sensitive': 541583,\n",
       " 'comments': 119530,\n",
       " 'nevertheless': 437577,\n",
       " 'hed': 232980,\n",
       " 'med': 399342,\n",
       " 'mug': 426309,\n",
       " 'asap': 46249,\n",
       " 'charger': 103677,\n",
       " 'awol': 53008,\n",
       " 'csi': 129836,\n",
       " 'tonight': 614598,\n",
       " 'fml': 195139,\n",
       " 'arms': 44834,\n",
       " 'sore': 565254,\n",
       " 'tennis': 595168,\n",
       " 'wonders': 659281,\n",
       " 'unhappy': 632496,\n",
       " 'split': 568774,\n",
       " 'seccond': 539687,\n",
       " 'saying': 535066,\n",
       " 'bye': 91263,\n",
       " 'statravelau': 572805,\n",
       " 'ur': 634747,\n",
       " 'newsletter': 438036,\n",
       " 'fares': 186238,\n",
       " 'unbelievable': 631391,\n",
       " 'shame': 544418,\n",
       " 'booked': 78166,\n",
       " 'paid': 463142,\n",
       " 'mine': 409668,\n",
       " 'missin': 411871,\n",
       " 'boo': 77917,\n",
       " 'markhardy1974': 392392,\n",
       " 'itm': 309737,\n",
       " 'damn': 134450,\n",
       " 'chalk': 102672,\n",
       " 'chalkboard': 102675,\n",
       " 'useless': 635429,\n",
       " 'blast': 73576,\n",
       " 'getty': 210182,\n",
       " 'villa': 641073,\n",
       " 'hates': 230098,\n",
       " 'shes': 547575,\n",
       " 'throat': 606327,\n",
       " 'worse': 661729,\n",
       " 'msdrama': 424515,\n",
       " 'sup': 582734,\n",
       " 'mama': 388465,\n",
       " 'tummy': 623095,\n",
       " 'hurts': 294818,\n",
       " 'wonder': 659147,\n",
       " 'hypnosis': 295714,\n",
       " 'anything': 41399,\n",
       " 'stop': 576456,\n",
       " 'smoking': 560193,\n",
       " 'fat': 186918,\n",
       " 'ones': 456090,\n",
       " 'januarycrimson': 315656,\n",
       " 'babe': 54846,\n",
       " 'fam': 185272,\n",
       " 'annoys': 39806,\n",
       " 'thankfully': 597269,\n",
       " 'theyre': 603769,\n",
       " 'muahaha': 425813,\n",
       " 'evil': 180469,\n",
       " 'laugh': 360500,\n",
       " 'hollywoodheat': 240870,\n",
       " 'attention': 49857,\n",
       " 'covered': 125571,\n",
       " 'photoshop': 474170,\n",
       " 'webpage': 649040,\n",
       " 'design': 146468,\n",
       " 'undergrad': 631864,\n",
       " 'wednesday': 649316,\n",
       " 'bday': 61382,\n",
       " '2': 4241,\n",
       " 'poor': 481329,\n",
       " 'cameron': 93944,\n",
       " 'hills': 238036,\n",
       " 'pray': 483906,\n",
       " 'please': 478600,\n",
       " 'ex': 181029,\n",
       " 'threatening': 606139,\n",
       " 'sh': 543666,\n",
       " 'myour': 429504,\n",
       " 'babies': 54962,\n",
       " '1st': 4099,\n",
       " 'birthday': 71435,\n",
       " 'party': 466068,\n",
       " 'jerk': 321891,\n",
       " 'headache': 231454,\n",
       " 'makeherfamous': 387573,\n",
       " 'hmm': 239358,\n",
       " 'enjoy': 174686,\n",
       " 'problems': 486686,\n",
       " 'constants': 121838,\n",
       " 'things': 604153,\n",
       " 'find': 191227,\n",
       " 'ulike': 630659,\n",
       " 'strider': 577761,\n",
       " 'little': 371746,\n",
       " 'puppy': 490014,\n",
       " 'httpappsfacebookcomdogbookprofileview5248435': 245620,\n",
       " 'ryleegracewana': 527615,\n",
       " 'steves': 575032,\n",
       " 'since': 553875,\n",
       " 'easter': 165637,\n",
       " 'wnt': 658566,\n",
       " 'able': 16199,\n",
       " 'ohh': 452418,\n",
       " 'actually': 18322,\n",
       " 'bracket': 81066,\n",
       " 'pools': 481174,\n",
       " 'wasnt': 646994,\n",
       " 'stark': 572100,\n",
       " 'follow': 195639,\n",
       " 'nite': 442842,\n",
       " 'favorite': 187384,\n",
       " 'teams': 593167,\n",
       " 'astros': 48844,\n",
       " 'spartans': 567321,\n",
       " 'lose': 377289,\n",
       " 'tw': 623981,\n",
       " 'missing': 411876,\n",
       " 'northern': 445862,\n",
       " 'calif': 93103,\n",
       " 'girl': 212049,\n",
       " 'police': 480411,\n",
       " 'remains': 513088,\n",
       " 'california': 93112,\n",
       " 'httptrimimji': 273283,\n",
       " 'mangaaa': 389448,\n",
       " 'hope': 242691,\n",
       " 'increase': 302890,\n",
       " 'capacity': 95451,\n",
       " 'fast': 186775,\n",
       " 'yesterday': 672829,\n",
       " 'pain': 463260,\n",
       " 'fail': 184481,\n",
       " 'whale': 652500,\n",
       " '15': 2702,\n",
       " 'behind': 64665,\n",
       " 'classes': 114018,\n",
       " 'quothousequot': 497047,\n",
       " 'kpreyes': 352605,\n",
       " 'remember': 513177,\n",
       " 'bum': 88795,\n",
       " 'leg': 364187,\n",
       " 'strikes': 577796,\n",
       " 'serious': 542162,\n",
       " 'paradisej': 465102,\n",
       " 'kinds': 348212,\n",
       " 'complaints': 120063,\n",
       " 'laptop': 359185,\n",
       " 'overheating': 461197,\n",
       " 'recalls': 509934,\n",
       " 'emily': 172255,\n",
       " 'mommy': 417172,\n",
       " 'training': 618422,\n",
       " 'misses': 411696,\n",
       " 'httpappsfacebookcomdogbookprofileview6176014': 245670,\n",
       " 'rather': 506996,\n",
       " 'send': 541365,\n",
       " 'messages': 403879,\n",
       " '3rd': 8454,\n",
       " 'mixed': 413755,\n",
       " 'sophmore': 565130,\n",
       " 'year': 671533,\n",
       " 'henkuyinepu': 234894,\n",
       " 'overrated': 461363,\n",
       " 'marykatherineq': 393936,\n",
       " 'wondered': 659168,\n",
       " 'thing': 604028,\n",
       " 'moscow': 420810,\n",
       " 'laying': 362140,\n",
       " 'voice': 642967,\n",
       " 'sooo': 564621,\n",
       " 'killed': 347177,\n",
       " 'kutner': 355165,\n",
       " 'house': 244194,\n",
       " 'whyyyyyyyy': 654972,\n",
       " 'jacobsummers': 312777,\n",
       " 'mea': 398711,\n",
       " 'culpa': 130529,\n",
       " 'alliana07': 29023,\n",
       " 'sense': 541530,\n",
       " 'suicide': 580797,\n",
       " 'refuse': 511869,\n",
       " 'believe': 65064,\n",
       " 'happened': 228371,\n",
       " 'salancaster': 529737,\n",
       " 'mercedesashley': 403233,\n",
       " 'grind': 220818,\n",
       " 'inspirational': 305220,\n",
       " 'saddening': 528616,\n",
       " 'cuz': 131823,\n",
       " 'hibanick': 237099,\n",
       " 'yeah': 671173,\n",
       " 'wudnt': 663573,\n",
       " 'chance': 102901,\n",
       " 'ugh': 629739,\n",
       " '130am': 2275,\n",
       " 'hanging': 227656,\n",
       " 'crooners': 128702,\n",
       " 'sing': 554007,\n",
       " 'sucks': 580031,\n",
       " 'erresc': 177237,\n",
       " 'aaw': 15349,\n",
       " 'bh': 68893,\n",
       " 'quotmorningquot': 498545,\n",
       " 'aww': 53133,\n",
       " 'beach': 61637,\n",
       " 'pissed': 476763,\n",
       " 'theres': 602617,\n",
       " 'asbas': 46286,\n",
       " 'radio': 504134,\n",
       " 'station': 572750,\n",
       " 'n': 430732,\n",
       " 'flipped': 193913,\n",
       " 'upside': 634496,\n",
       " 'head': 231439,\n",
       " 'ramen': 505809,\n",
       " 'sounds': 566089,\n",
       " 'sides': 552037,\n",
       " 'mention': 402980,\n",
       " 'crying': 129393,\n",
       " 'made': 385446,\n",
       " 'late': 359932,\n",
       " 'snack': 560604,\n",
       " 'glass': 213179,\n",
       " 'oj': 453357,\n",
       " 'bc': 61147,\n",
       " 'quotdown': 495430,\n",
       " 'sicknessquot': 551781,\n",
       " 'sleepugh': 557908,\n",
       " 'allyheman': 29773,\n",
       " 'big': 69527,\n",
       " 'fan': 185637,\n",
       " 'camilla': 94057,\n",
       " 'belle': 65397,\n",
       " 'grum': 221735,\n",
       " 'wah': 644880,\n",
       " 'clip': 115167,\n",
       " 'must': 427977,\n",
       " 'elstupido': 171367,\n",
       " 'filters': 190874,\n",
       " 'wait': 645109,\n",
       " 'till': 608267,\n",
       " 'puter': 490623,\n",
       " 'something': 563406,\n",
       " 'else': 171274,\n",
       " 'blame': 73383,\n",
       " 'broke': 85358,\n",
       " 'seems': 540225,\n",
       " 'longer': 375939,\n",
       " 'terms': 595640,\n",
       " 'cold': 117868,\n",
       " 'thecoolestout': 599168,\n",
       " 'ehhh': 168628,\n",
       " 'weathers': 648724,\n",
       " 'take': 588936,\n",
       " 'turn': 623419,\n",
       " 'chelserlynn': 105947,\n",
       " 'cooooold': 123119,\n",
       " 'incredible': 302922,\n",
       " 'stuff': 578630,\n",
       " 'hoping': 242858,\n",
       " 'rumbles': 526008,\n",
       " 'knights': 350844,\n",
       " 'notice': 446543,\n",
       " 'told': 613139,\n",
       " 'id': 298039,\n",
       " 'agency': 22014,\n",
       " 'said': 529313,\n",
       " 'bedtime': 63722,\n",
       " 'alive': 28426,\n",
       " 'yawwwnn': 670679,\n",
       " 'tired': 610080,\n",
       " 'imma': 301660,\n",
       " 'try': 621924,\n",
       " 'hopefully': 242735,\n",
       " 'headstart': 231835,\n",
       " 'aghsnow': 22289,\n",
       " 'kenny': 344229,\n",
       " 'powers': 483224,\n",
       " 'bridgetsbeaches': 83978,\n",
       " 'thank': 597252,\n",
       " 'letting': 366074,\n",
       " 'direct': 150819,\n",
       " 'message': 403857,\n",
       " 'bridget': 83946,\n",
       " 'india': 303136,\n",
       " '100th': 814,\n",
       " 'test': 596153,\n",
       " 'victory': 640448,\n",
       " '10th': 1311,\n",
       " 'consecutive': 121683,\n",
       " 'win': 656494,\n",
       " 'without': 657880,\n",
       " 'guess': 222597,\n",
       " 'ozesteph1992': 462121,\n",
       " 'stephan': 573768,\n",
       " 'mrsaintnick': 423438,\n",
       " 'leavin': 363376,\n",
       " 'intending': 305631,\n",
       " 'finish': 191536,\n",
       " 'editing': 167237,\n",
       " '536page': 10302,\n",
       " 'novel': 447040,\n",
       " 'manuscript': 390202,\n",
       " 'probably': 486567,\n",
       " 'happen': 228357,\n",
       " '12': 1780,\n",
       " 'pages': 463026,\n",
       " 'left': 364054,\n",
       " 'laid': 357637,\n",
       " 'twista202': 626003,\n",
       " 'havent': 230430,\n",
       " 'read': 508396,\n",
       " '9thamp10th': 14041,\n",
       " 'princess': 485859,\n",
       " 'diaries': 148691,\n",
       " 'saving': 534757,\n",
       " 'francesca': 198498,\n",
       " 'end': 173968,\n",
       " 'easy': 165711,\n",
       " 'books': 78299,\n",
       " 'nokia': 444342,\n",
       " '1110': 1403,\n",
       " 'died': 149145,\n",
       " 'mom': 416818,\n",
       " 'breast': 82692,\n",
       " 'cancer': 94629,\n",
       " 'worried': 661620,\n",
       " 'better': 67931,\n",
       " 'rumblepurr': 526007,\n",
       " 'understood': 631987,\n",
       " 'daylight': 140064,\n",
       " 'savings': 534764,\n",
       " 'ended': 174006,\n",
       " 'breakfast': 82427,\n",
       " 'keep': 342651,\n",
       " 'waking': 645438,\n",
       " 'onemoreproject': 455998,\n",
       " 'lame': 358357,\n",
       " 'understand': 631947,\n",
       " 'heroes': 235751,\n",
       " 'isnt': 308713,\n",
       " 'season': 539389,\n",
       " 'living': 372527,\n",
       " 'downtown': 158792,\n",
       " 'fun': 202891,\n",
       " 'jonathanchard': 329492,\n",
       " 'calorie': 93615,\n",
       " 'wise': 657464,\n",
       " 'junk': 334693,\n",
       " 'food': 196247,\n",
       " 'free': 199488,\n",
       " 'ate': 49121,\n",
       " 'sour': 566207,\n",
       " 'skittles': 556352,\n",
       " 'ass': 48227,\n",
       " 'cherry': 106332,\n",
       " 'coke': 117747,\n",
       " 'man': 388811,\n",
       " 'hard': 228936,\n",
       " 'hot': 243484,\n",
       " 'studying': 578505,\n",
       " 'sleeeep': 557383,\n",
       " 'eyebrows': 182999,\n",
       " 'waxed': 647788,\n",
       " 'phantasy': 472721,\n",
       " 'star': 571856,\n",
       " 'macheist': 384724,\n",
       " '30': 7028,\n",
       " 'apps': 42766,\n",
       " 'sweet': 585814,\n",
       " 'espresso': 177958,\n",
       " 'serial': 542104,\n",
       " 'although': 30928,\n",
       " 'sent': 541617,\n",
       " 'picked': 474786,\n",
       " 'mich': 405466,\n",
       " 'st': 570989,\n",
       " 'pretty': 485177,\n",
       " 'pick': 474776,\n",
       " 'way': 647816,\n",
       " 'untiltonight': 633561,\n",
       " 'alone': 30096,\n",
       " 'downstairsworking': 158756,\n",
       " 'ryanseacrest': 527430,\n",
       " 'anoop': 39955,\n",
       " 'mean': 398885,\n",
       " 'seriously': 542191,\n",
       " 'kinda': 348101,\n",
       " 'pinkserendipity': 476280,\n",
       " 'sprint': 569614,\n",
       " '4g': 9514,\n",
       " 'baltimore': 57757,\n",
       " 'chicago': 106994,\n",
       " 'far': 186154,\n",
       " 'stuck': 578260,\n",
       " 'awake': 52029,\n",
       " 'middle': 406766,\n",
       " 'second': 539720,\n",
       " 'row': 524377,\n",
       " 'felt': 188914,\n",
       " 'bursting': 89754,\n",
       " 'bubble': 87612,\n",
       " 'gosh': 217316,\n",
       " 'marieclr': 391542,\n",
       " 'naughtyhaughty': 434327,\n",
       " 'page': 462964,\n",
       " 'sooooo': 564688,\n",
       " 'deleted': 144280,\n",
       " 'history': 238869,\n",
       " 'crazy': 127087,\n",
       " 'wind': 656563,\n",
       " 'birding': 71258,\n",
       " 'httpffim1xtti': 258219,\n",
       " 'currently': 131129,\n",
       " 'grrr': 221529,\n",
       " 'ipods': 307226,\n",
       " 'acting': 18107,\n",
       " 'weird': 650537,\n",
       " 'jai': 313364,\n",
       " 'ho': 239811,\n",
       " 'thinking': 604385,\n",
       " 'arent': 43866,\n",
       " 'full': 202723,\n",
       " 'songs': 563917,\n",
       " 'ughh': 629808,\n",
       " 'penndbad': 470196,\n",
       " 'dvd': 163911,\n",
       " 'cos': 124255,\n",
       " 'heaps': 232009,\n",
       " 'deal': 141790,\n",
       " 'website': 649087,\n",
       " 'machineplay': 384772,\n",
       " 'therapyfail': 601969,\n",
       " 'colindemar': 118200,\n",
       " 'rail': 504896,\n",
       " 'tips': 610014,\n",
       " 'swear': 585623,\n",
       " 'losing': 377359,\n",
       " 'gaining': 205353,\n",
       " 'tweeps': 624325,\n",
       " 'wrenching': 662619,\n",
       " 'realized': 508947,\n",
       " 'hiding': 237263,\n",
       " 'staying': 572961,\n",
       " 'househouse': 244301,\n",
       " 'neighbors': 435929,\n",
       " 'loudhaving': 377835,\n",
       " 'dannyvegasbaby': 136628,\n",
       " 'danny': 136444,\n",
       " 'live': 372189,\n",
       " 'chat': 104528,\n",
       " 'car': 95893,\n",
       " '3': 7027,\n",
       " 'trip': 620492,\n",
       " 'soooo': 564660,\n",
       " 'check': 104996,\n",
       " 'httpwwwerikaobscurablogspotcom': 289978,\n",
       " 'borders': 79057,\n",
       " 'closed': 115412,\n",
       " '10': 626,\n",
       " 'downloading': 158645,\n",
       " 'nins': 442569,\n",
       " 'album': 26024,\n",
       " 'quotthe': 501508,\n",
       " 'slipquot': 558461,\n",
       " 'come': 119035,\n",
       " 'days': 140273,\n",
       " 'woke': 658800,\n",
       " 'written': 662934,\n",
       " 'email': 171615,\n",
       " 'early': 165177,\n",
       " 'university': 632828,\n",
       " 'teach': 592854,\n",
       " '830': 12762,\n",
       " 'hill': 237921,\n",
       " 'making': 387771,\n",
       " 'channels': 103247,\n",
       " 'yet': 673060,\n",
       " 'boring': 79371,\n",
       " 'lazy': 362302,\n",
       " 'hobby': 239888,\n",
       " 'supersport': 583435,\n",
       " 'buddy': 88004,\n",
       " 'ny': 449260,\n",
       " '25th': 5489,\n",
       " 'robluketic': 520754,\n",
       " 'french': 199988,\n",
       " 'south': 566272,\n",
       " 'qtr': 491673,\n",
       " 'snarl': 560826,\n",
       " 'beautiful': 62449,\n",
       " 'opps': 458110,\n",
       " 'remain': 513079,\n",
       " 'problem': 486647,\n",
       " 'activated': 18182,\n",
       " 'selfcontrol': 540780,\n",
       " 'block': 74493,\n",
       " 'meaning': 398943,\n",
       " 'qc': 491293,\n",
       " 'regularizing': 512169,\n",
       " 'internal': 305887,\n",
       " 'clock': 115295,\n",
       " 'difficult': 149466,\n",
       " 'fb': 187641,\n",
       " 'hillydop': 238100,\n",
       " 'spencer': 567981,\n",
       " 'guy': 223672,\n",
       " 'goodlaura': 216149,\n",
       " 'reese': 511515,\n",
       " 'dying': 164339,\n",
       " 'ttsc': 622526,\n",
       " 'finale': 190952,\n",
       " 'next': 438233,\n",
       " '24': 5172,\n",
       " 'madame': 385166,\n",
       " 'president': 484900,\n",
       " 'woman': 659021,\n",
       " 'limited': 369508,\n",
       " 'letterstoohope': 366055,\n",
       " 'guys': 223769,\n",
       " 'finei': 191344,\n",
       " 'dogsheã\\x83â\\x83ã\\x82â¯ã\\x83â\\x82ã\\x82â¿ã\\x83â\\x82ã\\x82â½s': 155502,\n",
       " 'shit': 548571,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sooo sad!!! they killed off Kutner on House  whyyyyyyyy\n",
      "\n",
      "\n",
      "  (0, 244194)\t1\n",
      "  (0, 300985)\t1\n",
      "  (0, 347177)\t1\n",
      "  (0, 355165)\t1\n",
      "  (0, 528526)\t1\n",
      "  (0, 564621)\t1\n",
      "  (0, 654972)\t1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of vectorized text\n",
    "sample_tweet = data['tweet'][111]\n",
    "print(sample_tweet)\n",
    "print('\\n')\n",
    "# vector representation\n",
    "bow_sample = bow_transformer.transform([sample_tweet])\n",
    "print(bow_sample)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (1198985, 686169)\n",
      "Amount of Non-Zero occurences:  9201789\n"
     ]
    }
   ],
   "source": [
    "# transform the entire DataFrame of messages\n",
    "messages_bow = bow_transformer.transform(data['tweet'])\n",
    "\n",
    "# check out the bag-of-words counts for the entire corpus as a large sparse matrix\n",
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Term Frequency, Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 654972)\t0.5728549514754353\n",
      "  (0, 564621)\t0.30086918392013245\n",
      "  (0, 528526)\t0.2319661525544924\n",
      "  (0, 355165)\t0.5430683528105762\n",
      "  (0, 347177)\t0.3717059598397657\n",
      "  (0, 300985)\t0.14977070356375646\n",
      "  (0, 244194)\t0.268303601831209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf_sample = tfidf_transformer.transform(bow_sample)\n",
    "print(tfidf_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1198985, 686169)\n"
     ]
    }
   ],
   "source": [
    "# to transform the entire bag-of-words corpus\n",
    "tweet_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(tweet_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 40.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 125.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 138.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: 0.533935 using {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "\n",
      "\n",
      "Mean: 0.524345 Stdev:(0.005149) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.526015 Stdev:(0.008175) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.519057 Stdev:(0.000766) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518440 Stdev:(0.000366) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.528067 Stdev:(0.008229) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.529299 Stdev:(0.008719) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.520888 Stdev:(0.001753) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.525403 Stdev:(0.008721) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.530729 Stdev:(0.004090) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.533935 Stdev:(0.009624) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.527506 Stdev:(0.008674) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.526489 Stdev:(0.006415) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.526593 Stdev:(0.009120) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.523667 Stdev:(0.001988) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.521017 Stdev:(0.004891) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.519978 Stdev:(0.001614) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.528325 Stdev:(0.006676) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.530291 Stdev:(0.008666) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.523120 Stdev:(0.006050) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.522669 Stdev:(0.006511) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.531000 Stdev:(0.005647) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.533047 Stdev:(0.008188) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.527100 Stdev:(0.007933) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.527835 Stdev:(0.007532) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.521425 Stdev:(0.002856) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.521798 Stdev:(0.007051) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.518096 Stdev:(0.000092) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518002 Stdev:(0.000074) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.523277 Stdev:(0.006615) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.521228 Stdev:(0.002104) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.518547 Stdev:(0.000634) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.519264 Stdev:(0.001746) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.522482 Stdev:(0.001834) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.522818 Stdev:(0.003137) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.519174 Stdev:(0.001673) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.521937 Stdev:(0.007002) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.519103 Stdev:(0.000695) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.519961 Stdev:(0.002357) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.518233 Stdev:(0.000476) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518445 Stdev:(0.000709) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.520105 Stdev:(0.001031) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.523009 Stdev:(0.004322) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.520180 Stdev:(0.002661) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518358 Stdev:(0.000392) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.521724 Stdev:(0.001755) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.521554 Stdev:(0.001940) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.519791 Stdev:(0.001570) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518961 Stdev:(0.000818) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', RandomForestClassifier()),  \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__n_estimators': [20, 50],\n",
    "                'clf__max_features': ['auto'],\n",
    "                'clf__max_depth' : [4,5,6],\n",
    "                'clf__criterion' :['gini', 'entropy'],\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed: 65.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: 0.836509 using {'bow__ngram_range': (1, 2), 'clf__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "\n",
      "\n",
      "Mean: 0.803009 Stdev:(0.001064) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.01, 'tfidf__use_idf': True}\n",
      "Mean: 0.818192 Stdev:(0.001022) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "Mean: 0.799113 Stdev:(0.000957) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.001, 'tfidf__use_idf': True}\n",
      "Mean: 0.812924 Stdev:(0.001147) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.001, 'tfidf__use_idf': False}\n",
      "Mean: 0.823180 Stdev:(0.001459) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.01, 'tfidf__use_idf': True}\n",
      "Mean: 0.836509 Stdev:(0.001448) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "Mean: 0.810987 Stdev:(0.001503) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.001, 'tfidf__use_idf': True}\n",
      "Mean: 0.825470 Stdev:(0.001383) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.001, 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', MultinomialNB()),  \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__alpha': (1e-2, 1e-3),\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_sentiment_NB.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save best model to current working directory\n",
    "joblib.dump(grid, \"twitter_sentiment_NB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.6min\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', SGDClassifier(class_weight='balanced')),  \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__loss' : [\"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "                'clf__alpha': (1e-3, 1e-4),\n",
    "                'clf__penalty' : [\"l2\", \"none\"],\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed: 113.2min\n",
      "[Parallel(n_jobs=8)]: Done 160 out of 160 | elapsed: 435.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: 0.812328 using {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': True}\n",
      "\n",
      "\n",
      "Mean: 0.752537 Stdev:(0.001417) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': True}\n",
      "Mean: 0.750133 Stdev:(0.001144) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': False}\n",
      "Mean: 0.752534 Stdev:(0.001417) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': True}\n",
      "Mean: 0.750134 Stdev:(0.001145) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': False}\n",
      "Mean: 0.746221 Stdev:(0.001099) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': True}\n",
      "Mean: 0.754174 Stdev:(0.001282) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': False}\n",
      "Mean: 0.746220 Stdev:(0.001102) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': True}\n",
      "Mean: 0.754176 Stdev:(0.001283) with: {'bow__ngram_range': (1, 1), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': False}\n",
      "Mean: 0.812328 Stdev:(0.001438) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': True}\n",
      "Mean: 0.811100 Stdev:(0.001388) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': False}\n",
      "Mean: 0.812319 Stdev:(0.001439) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': True}\n",
      "Mean: 0.811099 Stdev:(0.001391) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': False}\n",
      "Mean: 0.805967 Stdev:(0.000917) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': True}\n",
      "Mean: 0.808012 Stdev:(0.001282) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 0.0001, 'tfidf__use_idf': False}\n",
      "Mean: 0.805967 Stdev:(0.000917) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': True}\n",
      "Mean: 0.808011 Stdev:(0.001281) with: {'bow__ngram_range': (1, 2), 'clf__estimator__loss': 'squared_hinge', 'clf__estimator__penalty': 'l2', 'clf__estimator__tol': 1e-05, 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced', max_iter=100000))) \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1,1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__estimator__loss' : [\"hinge\", \"squared_hinge\"],\n",
    "                'clf__estimator__tol' : [1e-4, 1e-5],\n",
    "                'clf__estimator__penalty' : [\"l2\"]\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, verbose=1, n_jobs= 8)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_sentiment_LSVM_english.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save best model to current working directory\n",
    "joblib.dump(grid, \"twitter_sentiment_LSVM_english.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.8110693628360655\n",
      "\n",
      "\n",
      "confusion matrix: \n",
      " [[97379 19955   963]\n",
      " [21659 97073   399]\n",
      " [ 1217  1112    40]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82    118297\n",
      "           2       0.82      0.81      0.82    119131\n",
      "           4       0.03      0.02      0.02      2369\n",
      "\n",
      "    accuracy                           0.81    239797\n",
      "   macro avg       0.55      0.55      0.55    239797\n",
      "weighted avg       0.81      0.81      0.81    239797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_SVM = joblib.load(\"twitter_sentiment_LSVM_english.pkl\" )\n",
    "\n",
    "# get predictions from best model above\n",
    "y_preds = model_SVM.predict(X_test)\n",
    "\n",
    "print('accuracy score: ',accuracy_score(y_test, y_preds))\n",
    "print('\\n')\n",
    "print('confusion matrix: \\n',confusion_matrix(y_test,y_preds))\n",
    "print('\\n')\n",
    "print(metrics.classification_report(y_test, y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
