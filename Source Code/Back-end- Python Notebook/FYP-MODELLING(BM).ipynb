{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "import re, string\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "from IPython.display import IFrame\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster, FastMarkerCluster, HeatMapWithTime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kenapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>solange knowles marah kepada saya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>berbunyi pecah membuat saya menangis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>adalah kejutan Kate tidak memenangi pelatih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ketawa kuat. . . saya tidak percaya saya terle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554058</th>\n",
       "      <td>2</td>\n",
       "      <td>@spielpIatz hy sayang kenalan yu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554059</th>\n",
       "      <td>2</td>\n",
       "      <td>/wal aku cantik lah kelonya ku cantik?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554060</th>\n",
       "      <td>2</td>\n",
       "      <td>Wtb slogan! Stray kids hyunjin atau IN drop do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554061</th>\n",
       "      <td>2</td>\n",
       "      <td>1. 2017 - Anem\\r\\n2. 2017 - Keyla\\r\\n3. 2018 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554062</th>\n",
       "      <td>2</td>\n",
       "      <td>Orang yg hebat boleh menghasilkan sebuah karya...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1554063 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                              tweet\n",
       "0            0                                             kenapa\n",
       "1            0                  solange knowles marah kepada saya\n",
       "2            0               berbunyi pecah membuat saya menangis\n",
       "3            0        adalah kejutan Kate tidak memenangi pelatih\n",
       "4            0  ketawa kuat. . . saya tidak percaya saya terle...\n",
       "...        ...                                                ...\n",
       "1554058      2                   @spielpIatz hy sayang kenalan yu\n",
       "1554059      2             /wal aku cantik lah kelonya ku cantik?\n",
       "1554060      2  Wtb slogan! Stray kids hyunjin atau IN drop do...\n",
       "1554061      2  1. 2017 - Anem\\r\\n2. 2017 - Keyla\\r\\n3. 2018 -...\n",
       "1554062      2  Orang yg hebat boleh menghasilkan sebuah karya...\n",
       "\n",
       "[1554063 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label 0-negative, 2-means positive, 4-neutral\n",
    "data = pd.read_csv(\"trainingdata.csv\",skip_blank_lines=True,encoding = \"latin\") \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(df)\n",
    "data2.drop(data2.index[0:160000],0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative tagged sentences is: 637760\n",
      "number of positive tagged sentences is: 743846\n",
      "number of neutral tagged sentences  is: 12457\n",
      "total length of the data is:            1394063\n"
     ]
    }
   ],
   "source": [
    "# check the number of positive vs. negative tagged sentences\n",
    "negatives = data2['label'][data2.label == 0]\n",
    "positives = data2['label'][data2.label == 2]\n",
    "neutral = data2['label'][data2.label == 4]\n",
    "\n",
    "print('number of negative tagged sentences is: {}'.format(len(negatives)))\n",
    "print('number of positive tagged sentences is: {}'.format(len(positives)))\n",
    "print('number of neutral tagged sentences  is: {}'.format(len(neutral)))\n",
    "print('total length of the data is:            {}'.format(data2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data2)\n",
    "data.drop(data.index[1000000:1106086],0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative tagged sentences is: 637759\n",
      "number of positive tagged sentences is: 637760\n",
      "number of neutral tagged sentences  is: 12457\n",
      "total length of the data is:            1287976\n"
     ]
    }
   ],
   "source": [
    "# check the number of positive vs. negative tagged sentences\n",
    "negatives = data['label'][data.label == 0]\n",
    "positives = data['label'][data.label == 2]\n",
    "neutral = data['label'][data.label == 4]\n",
    "\n",
    "print('number of negative tagged sentences is: {}'.format(len(negatives)))\n",
    "print('number of positive tagged sentences is: {}'.format(len(positives)))\n",
    "print('number of neutral tagged sentences  is: {}'.format(len(neutral)))\n",
    "print('total length of the data is:            {}'.format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing 1:  Clean tweet text by removing links, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(tweet):\n",
    "    tweet = tweet.lower() #convert text to lower-case\n",
    "    tweet = re.sub(r'\\&\\w*;', '',tweet) # remove HTML special entities (e.g. &amp;)\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet) # remove tickers\n",
    "    tweet = re.sub(r'#\\w*', '', tweet) # remove hashtags\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet) # remove hyperlinks\n",
    "    tweet = tweet.lstrip(' ') # remove single space remaining at the front of the tweet.\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet) # remove whitespace (including new line characters) \n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet) # remove words with 2 or fewer letters \n",
    "    tweet = tweet.lstrip(' ') # remove single space remaining at the front of the tweet.\n",
    "  \n",
    "    # remove characters beyond Basic Multilingual Plane (BMP) of unicode (contains characters for almost all modern languages, and a large number of symbols):\n",
    "    tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n",
    "    return tweet\n",
    "\n",
    "    # clean dataframe's text column\n",
    "    data['tweet'] = data['tweet'].apply(text_cleaning)\n",
    "    # preview some cleaned tweets\n",
    "    data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    int64 \n",
      "tweet    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1185758, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates\n",
    "data = data.drop_duplicates('tweet')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pre-processing 2: Tokenize without out Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdul abdullah acara ada adalah ahmad air akan akhbar akhir aktiviti alam amat amerika anak anggota antara antarabangsa apa apabila april as asas asean asia asing atas atau australia awal awam bagaimanapun bagi bahagian bahan baharu bahawa baik bandar bank banyak barangan baru baru-baru bawah beberapa bekas beliau belum berada berakhir berbanding berdasarkan berharap berikutan berjaya berjumlah berkaitan berkata berkenaan berlaku bermula bernama bernilai bersama berubah besar bhd bidang bilion bn boleh bukan bulan bursa cadangan china dagangan dalam dan dana dapat dari daripada dasar datang datuk demikian dengan depan derivatives dewan di diadakan dibuka dicatatkan dijangka diniagakan dis disember ditutup dolar dr dua dunia ekonomi eksekutif eksport empat enam faedah feb global hadapan hanya harga hari hasil hingga hubungan ia iaitu ialah indeks india indonesia industri ini islam isnin isu itu jabatan jalan jan jawatan jawatankuasa jepun jika jualan juga julai jumaat jumlah jun juta kadar kalangan kali kami kata katanya kaunter kawasan ke keadaan kecil kedua kedua-dua kedudukan kekal kementerian kemudahan kenaikan kenyataan kepada kepentingan keputusan kerajaan kerana kereta kerja kerjasama kes keselamatan keseluruhan kesihatan ketika ketua keuntungan kewangan khamis kini kira-kira kita klci klibor komposit kontrak kos kuala kuasa kukuh kumpulan lagi lain langkah laporan lebih lepas lima lot luar lumpur mac mahkamah mahu majlis makanan maklumat malam malaysia mana manakala masa masalah masih masing-masing masyarakat mata media mei melalui melihat memandangkan memastikan membantu membawa memberi memberikan membolehkan membuat mempunyai menambah menarik menawarkan mencapai mencatatkan mendapat mendapatkan menerima menerusi mengadakan mengambil mengenai menggalakkan menggunakan mengikut mengumumkan mengurangkan meningkat meningkatkan menjadi menjelang menokok menteri menunjukkan menurut menyaksikan menyediakan mereka merosot merupakan mesyuarat minat minggu minyak modal mohd mudah mungkin naik najib nasional negara negara-negara negeri niaga nilai nov ogos okt oleh operasi orang pada pagi paling pameran papan para paras parlimen parti pasaran pasukan pegawai pejabat pekerja pelabur pelaburan pelancongan pelanggan pelbagai peluang pembangunan pemberita pembinaan pemimpin pendapatan pendidikan penduduk penerbangan pengarah pengeluaran pengerusi pengguna pengurusan peniaga peningkatan penting peratus perdagangan perdana peringkat perjanjian perkara perkhidmatan perladangan perlu permintaan perniagaan persekutuan persidangan pertama pertubuhan pertumbuhan perusahaan peserta petang pihak pilihan pinjaman polis politik presiden prestasi produk program projek proses proton pukul pula pusat rabu rakan rakyat ramai rantau raya rendah ringgit rumah sabah sahaja saham sama sarawak satu sawit saya sdn sebagai sebahagian sebanyak sebarang sebelum sebelumnya sebuah secara sedang segi sehingga sejak sekarang sektor sekuriti selain selama selasa selatan selepas seluruh semakin semalam semasa sementara semua semula sen sendiri seorang sepanjang seperti sept september serantau seri serta sesi setiap setiausaha sidang singapura sini sistem sokongan sri sudah sukan suku sumber supaya susut syarikat syed tahap tahun tan tanah tanpa tawaran teknologi telah tempat tempatan tempoh tenaga tengah tentang terbaik terbang terbesar terbuka terdapat terhadap termasuk tersebut terus tetapi thailand tiada tidak tiga timbalan timur tindakan tinggi tun tunai turun turut umno unit untuk untung urus usaha utama walaupun wang wanita wilayah yang\n"
     ]
    }
   ],
   "source": [
    "with open('stopwords-ms.txt','r') as f2:\n",
    "   b=f2.read().split()\n",
    "   print(' '.join(x.lower() for x in b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nanan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160000</th>\n",
       "      <td>0</td>\n",
       "      <td>hari ini. entah bagaimana ia tidak kelihatan sangat penting. hidup agak sucks sekarang. saya perlukan beberapa rancangan.</td>\n",
       "      <td>[entah, bagaimana, kelihatan, sangat, hidup, agak, sucks, perlukan, rancangan]</td>\n",
       "      <td>[entah, bagaimana, kelihatan, sangat, hidup, agak, sucks, perlukan, rancangan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160001</th>\n",
       "      <td>0</td>\n",
       "      <td>marah. saya sangat tidak suka ineptitude dan cenderung berpandangan yang keras kepala. tuhan yang sayang, tolong saya daripada membunuh sesiapa sahaja di tempat kerja</td>\n",
       "      <td>[marah, sangat, suka, ineptitude, cenderung, berpandangan, keras, kepala, tuhan, sayang, tolong, membunuh, sesiapa]</td>\n",
       "      <td>[marah, sangat, suka, ineptitude, cenderung, berpandangan, keras, kepala, tuhan, sayang, tolong, membunuh, sesiapa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160002</th>\n",
       "      <td>0</td>\n",
       "      <td>ia figgers. satu hari minggu ini saya boleh pulang pada pukul 6 petang, dan lalu lintas menghisap sekarang terperangkap di pejabat yang merasa bersalah kerana tidak bekerja. lt</td>\n",
       "      <td>[figgers, pulang, 6, lalu, lintas, menghisap, terperangkap, merasa, bersalah, bekerja, lt]</td>\n",
       "      <td>[figgers, pulang, 6, lalu, lintas, menghisap, terperangkap, merasa, bersalah, bekerja, lt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160003</th>\n",
       "      <td>0</td>\n",
       "      <td>pagi esok. . . matahari, tidak hujan setakat ini. . . sakit kepala alergi lebih baik, tetapi tidak benar-benar hilang. . . akan berehat hari ini. . . ingin saya boleh duduk di luar</td>\n",
       "      <td>[esok, matahari, hujan, setakat, sakit, kepala, alergi, benarbenar, hilang, berehat, ingin, duduk]</td>\n",
       "      <td>[esok, matahari, hujan, setakat, sakit, kepala, alergi, benarbenar, hilang, berehat, ingin, duduk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160004</th>\n",
       "      <td>0</td>\n",
       "      <td>8. 5 jam lagi. . . .</td>\n",
       "      <td>[8, 5, jam]</td>\n",
       "      <td>[8, 5, jam]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  \\\n",
       "160000  0       \n",
       "160001  0       \n",
       "160002  0       \n",
       "160003  0       \n",
       "160004  0       \n",
       "\n",
       "                                                                                                                                                                                       tweet  \\\n",
       "160000  hari ini. entah bagaimana ia tidak kelihatan sangat penting. hidup agak sucks sekarang. saya perlukan beberapa rancangan.                                                              \n",
       "160001  marah. saya sangat tidak suka ineptitude dan cenderung berpandangan yang keras kepala. tuhan yang sayang, tolong saya daripada membunuh sesiapa sahaja di tempat kerja                 \n",
       "160002  ia figgers. satu hari minggu ini saya boleh pulang pada pukul 6 petang, dan lalu lintas menghisap sekarang terperangkap di pejabat yang merasa bersalah kerana tidak bekerja. lt       \n",
       "160003  pagi esok. . . matahari, tidak hujan setakat ini. . . sakit kepala alergi lebih baik, tetapi tidak benar-benar hilang. . . akan berehat hari ini. . . ingin saya boleh duduk di luar   \n",
       "160004  8. 5 jam lagi. . . .                                                                                                                                                                   \n",
       "\n",
       "                                                                                                                     tokens  \\\n",
       "160000  [entah, bagaimana, kelihatan, sangat, hidup, agak, sucks, perlukan, rancangan]                                        \n",
       "160001  [marah, sangat, suka, ineptitude, cenderung, berpandangan, keras, kepala, tuhan, sayang, tolong, membunuh, sesiapa]   \n",
       "160002  [figgers, pulang, 6, lalu, lintas, menghisap, terperangkap, merasa, bersalah, bekerja, lt]                            \n",
       "160003  [esok, matahari, hujan, setakat, sakit, kepala, alergi, benarbenar, hilang, berehat, ingin, duduk]                    \n",
       "160004  [8, 5, jam]                                                                                                           \n",
       "\n",
       "                                                                                                                      words  \n",
       "160000  [entah, bagaimana, kelihatan, sangat, hidup, agak, sucks, perlukan, rancangan]                                       \n",
       "160001  [marah, sangat, suka, ineptitude, cenderung, berpandangan, keras, kepala, tuhan, sayang, tolong, membunuh, sesiapa]  \n",
       "160002  [figgers, pulang, 6, lalu, lintas, menghisap, terperangkap, merasa, bersalah, bekerja, lt]                           \n",
       "160003  [esok, matahari, hujan, setakat, sakit, kepala, alergi, benarbenar, hilang, berehat, ingin, duduk]                   \n",
       "160004  [8, 5, jam]                                                                                                          "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize helper function\n",
    "def process_text(text):\n",
    "    nopunc = [char for char in list(text) if char not in string.punctuation] # check characters to see if they are in punctuation\n",
    "    nopunc = ''.join(nopunc) # join the characters again to form the string\n",
    "    return [word for word in nopunc.lower().split() if word.lower() not in b] # remove any stopwords\n",
    "\n",
    "def remove_words(word_list):\n",
    "    remove = ['com','pic','twitter','...','“','”','’','…']\n",
    "    return [w for w in word_list if w not in remove]\n",
    "\n",
    "# tokenize message column and create a column for tokens\n",
    "data = data.copy()\n",
    "data['tokens'] = data['tweet'].apply(process_text)\n",
    "data['words'] = data['tokens'].apply(remove_words)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000    [entah, bagaimana, kelihatan, sangat, hidup, agak, sucks, perlukan, rancangan]                                     \n",
       "160001    [marah, sangat, suka, ineptitude, cenderung, berpandangan, keras, kepala, tuhan, sayang, tolong, membunuh, sesiapa]\n",
       "160002    [figgers, pulang, 6, lalu, lintas, menghisap, terperangkap, merasa, bersalah, bekerja, lt]                         \n",
       "160003    [esok, matahari, hujan, setakat, sakit, kepala, alergi, benarbenar, hilang, berehat, ingin, duduk]                 \n",
       "160004    [8, 5, jam]                                                                                                        \n",
       "160005    [menjual, festival, pekan, twitter]                                                                                \n",
       "160006    [ohh, sedih, suka]                                                                                                 \n",
       "160007    [sooo, letih, ingin, bercakap, seseorang, terlepas, begitu]                                                        \n",
       "160008    [pemarah, tahu, berapa, menyakiti]                                                                                 \n",
       "160009    [terminator, sebenarnya, filem, walau, gilirannya, soo, panggilannya]                                              \n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# style 1 tokens\n",
    "data['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pre-processing 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorize (convert each message which is represented by a list of tokens into a vector that a machine learning model can understand)\n",
    "# converts a collection of text documents to a matrix of token counts\n",
    "bow_transformer = CountVectorizer(analyzer=process_text).fit(data['tweet']) \n",
    "# print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entah': 163202,\n",
       " 'bagaimana': 66657,\n",
       " 'kelihatan': 584569,\n",
       " 'sangat': 769584,\n",
       " 'hidup': 213136,\n",
       " 'agak': 27115,\n",
       " 'sucks': 809281,\n",
       " 'perlukan': 721472,\n",
       " 'rancangan': 747536,\n",
       " 'marah': 631592,\n",
       " 'suka': 810154,\n",
       " 'ineptitude': 548091,\n",
       " 'cenderung': 108669,\n",
       " 'berpandangan': 82507,\n",
       " 'keras': 587761,\n",
       " 'kepala': 586912,\n",
       " 'tuhan': 845602,\n",
       " 'sayang': 772524,\n",
       " 'tolong': 840396,\n",
       " 'membunuh': 641973,\n",
       " 'sesiapa': 784566,\n",
       " 'figgers': 176382,\n",
       " 'pulang': 737267,\n",
       " '6': 15045,\n",
       " 'lalu': 608905,\n",
       " 'lintas': 617257,\n",
       " 'menghisap': 646784,\n",
       " 'terperangkap': 831181,\n",
       " 'merasa': 650534,\n",
       " 'bersalah': 82992,\n",
       " 'bekerja': 76057,\n",
       " 'lt': 621291,\n",
       " 'esok': 165143,\n",
       " 'matahari': 634706,\n",
       " 'hujan': 538116,\n",
       " 'setakat': 784805,\n",
       " 'sakit': 767124,\n",
       " 'alergi': 36454,\n",
       " 'benarbenar': 77225,\n",
       " 'hilang': 213742,\n",
       " 'berehat': 79639,\n",
       " 'ingin': 548720,\n",
       " 'duduk': 153597,\n",
       " '8': 16947,\n",
       " '5': 13854,\n",
       " 'jam': 559373,\n",
       " 'menjual': 648319,\n",
       " 'festival': 175574,\n",
       " 'pekan': 713229,\n",
       " 'twitter': 848505,\n",
       " 'ohh': 698797,\n",
       " 'sedih': 776179,\n",
       " 'sooo': 801990,\n",
       " 'letih': 614667,\n",
       " 'bercakap': 79144,\n",
       " 'seseorang': 784515,\n",
       " 'terlepas': 830525,\n",
       " 'begitu': 75651,\n",
       " 'pemarah': 714281,\n",
       " 'tahu': 819645,\n",
       " 'berapa': 78473,\n",
       " 'menyakiti': 649216,\n",
       " 'terminator': 830742,\n",
       " 'sebenarnya': 775225,\n",
       " 'filem': 176729,\n",
       " 'walau': 862321,\n",
       " 'gilirannya': 191975,\n",
       " 'soo': 801869,\n",
       " 'panggilannya': 708279,\n",
       " 'oops': 701841,\n",
       " 'cutoff': 123902,\n",
       " 'makan': 627131,\n",
       " 'aplikasi': 50686,\n",
       " '3': 10696,\n",
       " 'jadi': 557749,\n",
       " 'lapar': 610215,\n",
       " 't': 818401,\n",
       " 'i': 540432,\n",
       " 'e': 155829,\n",
       " 'd': 124440,\n",
       " 'apaapa': 49750,\n",
       " 'sahajalah': 766267,\n",
       " 'v': 856470,\n",
       " 'gementar': 189225,\n",
       " 'dengannya': 132358,\n",
       " 'dia': 134961,\n",
       " 'weired': 865627,\n",
       " 'thanx': 833654,\n",
       " 'tapi': 822813,\n",
       " 'penjaga': 718009,\n",
       " 'merasakan': 650542,\n",
       " 'dihukum': 139173,\n",
       " 'kemudian': 585873,\n",
       " 'kembali': 585375,\n",
       " 'memuat': 642993,\n",
       " 'migrain': 653993,\n",
       " 'muzik': 668804,\n",
       " 'segalagala': 776702,\n",
       " 'menyakitkan': 649222,\n",
       " 'menunggu': 649052,\n",
       " 'lukess': 622299,\n",
       " 'mula': 666550,\n",
       " 'bermain': 82015,\n",
       " 'yay': 876999,\n",
       " 'nsw': 692502,\n",
       " 'kehilangan': 582998,\n",
       " 'jejak': 562609,\n",
       " 'kaki': 574219,\n",
       " 'ketawa': 589769,\n",
       " 'kuat': 602860,\n",
       " 'benci': 77311,\n",
       " 'barf': 71484,\n",
       " 'pernah': 721738,\n",
       " 'mempercayainya': 642648,\n",
       " 'roda': 759517,\n",
       " 'ksyrium': 602348,\n",
       " 'elit': 160210,\n",
       " 'mavic': 635840,\n",
       " 'pembawa': 714458,\n",
       " 'garpu': 187260,\n",
       " 'loon': 619918,\n",
       " 'tasik': 823486,\n",
       " 'tapak': 822762,\n",
       " 'ganjil': 186710,\n",
       " 'sendirinya': 781625,\n",
       " 'panggilan': 708270,\n",
       " 'keluar': 584833,\n",
       " 'pasangan': 710312,\n",
       " 'rakaman': 746561,\n",
       " 'darn': 128038,\n",
       " 'sekolah': 778367,\n",
       " 'hmph': 215396,\n",
       " 'nah': 672050,\n",
       " 'akhirnya': 33546,\n",
       " 'terlupa': 830626,\n",
       " 'kekunci': 583946,\n",
       " 'tertekan': 831938,\n",
       " 'kecewa': 581765,\n",
       " 'menangis': 643426,\n",
       " 'tanda': 821878,\n",
       " 'kolej': 597922,\n",
       " 'rumpai': 763335,\n",
       " 'percaya': 720031,\n",
       " 'memberitahu': 641615,\n",
       " 'pernahkah': 721752,\n",
       " 'anda': 45173,\n",
       " 'menyukai': 650044,\n",
       " 'lagu': 607520,\n",
       " 'video': 858609,\n",
       " '4': 12467,\n",
       " 'kamu': 575906,\n",
       " 'ya': 875007,\n",
       " 'terjadi': 829871,\n",
       " '2': 7282,\n",
       " 'bot': 95069,\n",
       " 'literal': 617788,\n",
       " 'omg': 700543,\n",
       " 'cetakan': 109533,\n",
       " 'sentiasa': 782283,\n",
       " 'separuh': 782828,\n",
       " 'hospital': 217283,\n",
       " 'setengah': 784984,\n",
       " 'kriket': 601476,\n",
       " 'merampas': 650439,\n",
       " 'katakan': 578728,\n",
       " 'quotpaybacks': 743147,\n",
       " 'baby': 65362,\n",
       " 'quot': 741279,\n",
       " 'jp': 569456,\n",
       " 'tergesagesa': 829410,\n",
       " 'sejuk': 777638,\n",
       " 'buruk': 101870,\n",
       " 'pergi': 720579,\n",
       " 'sukar': 810313,\n",
       " 'espero': 165227,\n",
       " 'que': 740914,\n",
       " 'nat': 675606,\n",
       " 'conteste': 119642,\n",
       " 'a': 18509,\n",
       " 'ver': 857949,\n",
       " 'si': 791354,\n",
       " 'vamos': 856905,\n",
       " 'al': 35296,\n",
       " 'hotel': 217388,\n",
       " 'now': 691189,\n",
       " 'or': 702570,\n",
       " 'what': 866566,\n",
       " 'mandi': 630103,\n",
       " 'dulu': 154036,\n",
       " 'me': 637841,\n",
       " 'saca': 765457,\n",
       " 'las': 610702,\n",
       " 'nauseas': 676110,\n",
       " 'aoche': 49554,\n",
       " 'vomite': 860200,\n",
       " 'menaiki': 643235,\n",
       " 'perjalanan': 721009,\n",
       " 'ibu': 541290,\n",
       " 'melakukannya': 639421,\n",
       " 'menuju': 648957,\n",
       " 'arah': 51892,\n",
       " 'terpaksa': 831051,\n",
       " 'kelas': 584276,\n",
       " 'bus': 101942,\n",
       " 'lewat': 614913,\n",
       " 'bodoh': 92578,\n",
       " 'terlalu': 830418,\n",
       " 'blergh': 90764,\n",
       " 'iklan': 544265,\n",
       " 'patut': 711249,\n",
       " 'dicukupi': 137480,\n",
       " 'biasanya': 86858,\n",
       " 'taitv': 819933,\n",
       " 'bau': 72842,\n",
       " 'bawang': 72999,\n",
       " 'putih': 738795,\n",
       " 'carian': 106336,\n",
       " 'friendfeed': 182091,\n",
       " 'mengingati': 646969,\n",
       " 'oh': 698703,\n",
       " 'bagus': 66973,\n",
       " 'semalaman': 779930,\n",
       " 'hubby': 537702,\n",
       " 'boston': 95047,\n",
       " 'bergerak': 80000,\n",
       " 'memenangi': 642145,\n",
       " 'loteri': 620369,\n",
       " 'keduaduanya': 582432,\n",
       " 'tangan': 821985,\n",
       " 'syria': 817844,\n",
       " 'mengikuti': 646905,\n",
       " 'siapa': 791650,\n",
       " 'loozers': 620066,\n",
       " 'pulau': 737312,\n",
       " 'weezer': 865412,\n",
       " 'membuatkan': 641892,\n",
       " 'menyenangkan': 649663,\n",
       " 'harapkan': 205921,\n",
       " 'melakukan': 639417,\n",
       " 'seni': 781943,\n",
       " 'sambil': 768572,\n",
       " 'menonton': 648461,\n",
       " 'musim': 668064,\n",
       " 'lelaki': 613252,\n",
       " 'keluarga': 584847,\n",
       " 'harus': 207105,\n",
       " 'sinar': 793842,\n",
       " 'fikiran': 176528,\n",
       " 'penumpang': 718606,\n",
       " 'france': 181042,\n",
       " 'udara': 850331,\n",
       " 'kotoran': 600669,\n",
       " 'telanjang': 826300,\n",
       " 'bubur': 99259,\n",
       " 'rasa': 748252,\n",
       " 'sedikit': 776310,\n",
       " 'terinspirasi': 829830,\n",
       " 'kentut': 586682,\n",
       " 'kek': 583580,\n",
       " 'ffff': 175757,\n",
       " 'harap': 205861,\n",
       " 'awak': 60857,\n",
       " 'ok': 699393,\n",
       " 'girlie': 192512,\n",
       " 'mengajar': 645078,\n",
       " 'terakhir': 828195,\n",
       " 'sharfmans': 787971,\n",
       " 'bangun': 70284,\n",
       " 'hahahaha': 201369,\n",
       " 'mati': 635106,\n",
       " 'mcdonalds': 637257,\n",
       " 'okay': 699437,\n",
       " 'lama': 608984,\n",
       " 'bilabila': 87828,\n",
       " 'belajar': 76241,\n",
       " 'temu': 827233,\n",
       " 'ramah': 747070,\n",
       " 'mahukan': 626394,\n",
       " 'pekerjaan': 713315,\n",
       " 'ughhh': 850655,\n",
       " 'lapang': 610192,\n",
       " 'mendapatkannya': 644171,\n",
       " 'dilakukan': 140925,\n",
       " 'merancang': 650457,\n",
       " 'celebs': 108299,\n",
       " 'masamasa': 633469,\n",
       " 'ujian': 851070,\n",
       " 'saraf': 770522,\n",
       " 'tidur': 836332,\n",
       " 'crap': 121263,\n",
       " 'panas': 707724,\n",
       " 'kegilaan': 582805,\n",
       " 'mutlak': 668574,\n",
       " 'menggugurkan': 646426,\n",
       " 'subjek': 808911,\n",
       " 'facebook': 168492,\n",
       " 'sepatutnya': 782877,\n",
       " 'dipanggil': 143069,\n",
       " 'spambook': 803235,\n",
       " 'spam': 803227,\n",
       " 'myspace': 669993,\n",
       " 'grrrr': 197120,\n",
       " 'sarapan': 770769,\n",
       " 'bersiap': 83424,\n",
       " 'sedia': 776156,\n",
       " 'kemarin': 585270,\n",
       " 'joffrey': 567846,\n",
       " 'fikir': 176522,\n",
       " 'mendengar': 644295,\n",
       " 'retak': 753941,\n",
       " 'tulang': 846070,\n",
       " 'belakang': 76298,\n",
       " 'bachaaaaaao': 65714,\n",
       " 'poppy': 730916,\n",
       " 'mengunyah': 647607,\n",
       " 'snek': 799462,\n",
       " 'anjing': 47547,\n",
       " 'tracksuit': 842174,\n",
       " 'rahsia': 745937,\n",
       " 'victoria': 858558,\n",
       " 'backordered': 65834,\n",
       " 'ambulans': 41047,\n",
       " 'kawan': 579452,\n",
       " 'tengahtengah': 827474,\n",
       " 'sejarah': 777488,\n",
       " 'bulubulu': 100763,\n",
       " 'johnny': 568101,\n",
       " 'bahasa': 67321,\n",
       " 'sepanyol': 782805,\n",
       " 'tinggal': 837453,\n",
       " 'maka': 627072,\n",
       " 'kelulusan': 584978,\n",
       " 'saudara': 771919,\n",
       " 'webinar': 865062,\n",
       " 'brosur': 97695,\n",
       " 'agar': 27276,\n",
       " 'sesuai': 784646,\n",
       " 'penjenamaan': 718193,\n",
       " 'mencukupi': 644054,\n",
       " 'menjadikannya': 648029,\n",
       " 'pengebumian': 716835,\n",
       " 'mesti': 651778,\n",
       " 'yakin': 875787,\n",
       " 'pautan': 711376,\n",
       " 'telefon': 826359,\n",
       " 'terima': 829703,\n",
       " 'kasih': 578404,\n",
       " 'mencari': 643771,\n",
       " 'sedar': 775999,\n",
       " 'melemparkannya': 640037,\n",
       " 'cuba': 122615,\n",
       " 'membersihkan': 641669,\n",
       " 'pantai': 708711,\n",
       " 'hi': 212773,\n",
       " 'nick': 684931,\n",
       " 'minta': 656037,\n",
       " 'maaf': 624262,\n",
       " 'komen': 598182,\n",
       " 'buat': 98970,\n",
       " 'ampun': 43674,\n",
       " 'askar': 56392,\n",
       " 'hormat': 217095,\n",
       " 'amp': 43292,\n",
       " 'hehehehe': 210139,\n",
       " 'moring': 662230,\n",
       " 'bermaksud': 82038,\n",
       " 'membosankan': 641853,\n",
       " 'hehehe': 210129,\n",
       " 'kadangkadang': 573039,\n",
       " 'kapal': 577069,\n",
       " 'menghargai': 646622,\n",
       " 'jenaka': 563320,\n",
       " 'mengapa': 645391,\n",
       " 'web': 865008,\n",
       " 'labahlabah': 606641,\n",
       " 'rahmat': 745871,\n",
       " 'tina': 837337,\n",
       " 'margo': 632030,\n",
       " 'lizh': 618269,\n",
       " 'lesley': 614539,\n",
       " 'hati': 207756,\n",
       " 'sebab': 774824,\n",
       " 'mengubahnya': 647439,\n",
       " 'jerman': 564321,\n",
       " 'mampg': 629533,\n",
       " 'jauh': 561106,\n",
       " 'permainan': 721511,\n",
       " 'kecuali': 582012,\n",
       " 'aku': 34434,\n",
       " 'membakar': 641241,\n",
       " 'diri': 144511,\n",
       " 'pmqs': 728807,\n",
       " 'kecenderungan': 581708,\n",
       " 'ugh': 850636,\n",
       " 'im': 545399,\n",
       " 'belanjawan': 76412,\n",
       " 'undieshopstop': 852927,\n",
       " 'ikut': 544548,\n",
       " '7': 15973,\n",
       " 'pengikut': 717414,\n",
       " 'menyedihkan': 649487,\n",
       " 'sibuk': 791948,\n",
       " 'kebanyakan': 580955,\n",
       " 'kuota': 604527,\n",
       " 'sekurangkurangnya': 778658,\n",
       " 'memeriksa': 642238,\n",
       " 'menyedari': 649473,\n",
       " 'balasan': 68460,\n",
       " 'bertanya': 83908,\n",
       " 'tiba': 835992,\n",
       " 'tiket': 836789,\n",
       " 'muat': 664882,\n",
       " 'sophs': 802188,\n",
       " 'mee': 638382,\n",
       " 'cant': 105402,\n",
       " 'shes': 789050,\n",
       " 'mengantuk': 645359,\n",
       " 'membalas': 641251,\n",
       " 'berundur': 84461,\n",
       " 'selamat': 778852,\n",
       " 'membelanjakan': 641491,\n",
       " 'buh': 99919,\n",
       " 'o': 697333,\n",
       " 'wwdc': 871985,\n",
       " 'sial': 791460,\n",
       " 'pc': 711753,\n",
       " 'howard': 217721,\n",
       " 'brooke': 97610,\n",
       " 'hogan': 215932,\n",
       " 'wheres': 866851,\n",
       " 'gibert': 191433,\n",
       " 'susan': 813056,\n",
       " 'boyle': 95502,\n",
       " 'makeover': 627552,\n",
       " 'pandai': 707944,\n",
       " 'membaca': 641166,\n",
       " 'bio': 88713,\n",
       " 'graduate': 195993,\n",
       " 'pengalaman': 716592,\n",
       " 'mengerikan': 645969,\n",
       " 'doc': 149264,\n",
       " 'pendaftaran': 715976,\n",
       " 'sehari': 777068,\n",
       " 'menang': 643367,\n",
       " 'jawapan': 561360,\n",
       " 'menyeru': 649767,\n",
       " 'mengembara': 645816,\n",
       " 'suan': 808594,\n",
       " 'lum': 622487,\n",
       " 'kawankawan': 579493,\n",
       " 'bertentangan': 84083,\n",
       " 'sihat': 792537,\n",
       " 'sekali': 777753,\n",
       " 'kopi': 599635,\n",
       " 'pembersihan': 714598,\n",
       " 'kod': 597391,\n",
       " 'selesai': 779319,\n",
       " 'jenis': 563547,\n",
       " 'timbunan': 837069,\n",
       " 'gatalgatal': 187539,\n",
       " 'manamana': 629857,\n",
       " 'gigitan': 191649,\n",
       " 'nyamuk': 695836,\n",
       " 'grrrrrr': 197127,\n",
       " 'emel': 161281,\n",
       " 'yettt': 878664,\n",
       " 'moi': 660707,\n",
       " 'ai': 29143,\n",
       " 'yai': 875662,\n",
       " 'hierarki': 213249,\n",
       " 'linear': 616989,\n",
       " 'pemodelan': 715225,\n",
       " 'desisater': 133426,\n",
       " 'lengkap': 613967,\n",
       " 'shoeshopping': 790148,\n",
       " 'kebodohan': 581355,\n",
       " 'brooklyn': 97622,\n",
       " 'paul': 711301,\n",
       " 'auster': 60051,\n",
       " 'buku': 100427,\n",
       " 'npe': 691287,\n",
       " 'perlahan': 721330,\n",
       " 'cara': 106056,\n",
       " 'menangkap': 643474,\n",
       " 'cuaca': 122562,\n",
       " 'boo': 94114,\n",
       " 'y': 874934,\n",
       " 'b': 64691,\n",
       " 'n': 670295,\n",
       " 'hotlanta': 217455,\n",
       " 'sherry': 789026,\n",
       " 'berry': 82907,\n",
       " 'ah': 28007,\n",
       " 'footy': 180220,\n",
       " 'footage': 180177,\n",
       " 'blues': 91511,\n",
       " 'hoo': 216736,\n",
       " 'ingat': 548592,\n",
       " 'kris': 601542,\n",
       " 'adam': 22569,\n",
       " 'awan': 61025,\n",
       " 'langit': 609738,\n",
       " 'melukis': 640638,\n",
       " 'warnawarna': 863804,\n",
       " 'lebam': 612199,\n",
       " 'mm': 658461,\n",
       " 'thats': 833755,\n",
       " 'menyebalkan': 649440,\n",
       " 'kesakitan': 588715,\n",
       " 'betulbetul': 85531,\n",
       " 'beritahu': 80616,\n",
       " 'mendarat': 644183,\n",
       " 'flip': 179030,\n",
       " 'lupa': 622764,\n",
       " 'germany': 190339,\n",
       " 'bimbang': 88227,\n",
       " 'nombor': 689295,\n",
       " 'digunakan': 138832,\n",
       " 'fuck': 183083,\n",
       " 'tertanyatanya': 831901,\n",
       " 'persembahan': 722154,\n",
       " 'hebat': 209724,\n",
       " 'terjebak': 829955,\n",
       " 'tho': 835346,\n",
       " 'yer': 878323,\n",
       " 'jodie': 567657,\n",
       " 'canny': 105366,\n",
       " 'takut': 820969,\n",
       " 'haha': 201332,\n",
       " 'theres': 834704,\n",
       " 'tibatiba': 836043,\n",
       " 'wellingtonians': 865815,\n",
       " 'sialan': 791475,\n",
       " 'masuk': 634446,\n",
       " 'akal': 33216,\n",
       " 'nasihat': 675227,\n",
       " 'lakukan': 608622,\n",
       " 'hendak': 211004,\n",
       " 'omelet': 700485,\n",
       " 'mengedit': 645636,\n",
       " 'jo': 567500,\n",
       " 'saja': 766784,\n",
       " 'mod': 659922,\n",
       " 'chem': 111583,\n",
       " 'menyemak': 649608,\n",
       " 'klasik': 595723,\n",
       " '2moro': 10296,\n",
       " 'adil': 23924,\n",
       " 'x': 872224,\n",
       " 'purata': 738156,\n",
       " 'pembayar': 714467,\n",
       " 'cukai': 122927,\n",
       " 'hutang': 539103,\n",
       " '50': 13855,\n",
       " '000': 2,\n",
       " 'pemilih': 715043,\n",
       " 'dipilih': 143744,\n",
       " 'tlc': 838967,\n",
       " 'jon': 568711,\n",
       " 'lapan': 610189,\n",
       " 'apartmen': 50110,\n",
       " 'realiti': 750114,\n",
       " 'tv': 847511,\n",
       " 'devo': 133976,\n",
       " 'kenapa': 586112,\n",
       " 'jb': 561736,\n",
       " 'mahal': 625945,\n",
       " 'komputer': 598518,\n",
       " 'riba': 755228,\n",
       " 'penyerahannya': 719016,\n",
       " 'kertas': 588490,\n",
       " 'persaraan': 722066,\n",
       " 'tadi': 818896,\n",
       " 'semoga': 780725,\n",
       " 'well': 865793,\n",
       " 'terutama': 832392,\n",
       " 'pemain': 714118,\n",
       " 'naga': 671943,\n",
       " 'nasib': 675174,\n",
       " 'melangkaui': 639552,\n",
       " 'arus': 54952,\n",
       " 'berjalan': 80722,\n",
       " 'penjara': 718058,\n",
       " 'pta': 736196,\n",
       " 'loya': 620974,\n",
       " 'fudge': 183241,\n",
       " 'mengganggu': 646117,\n",
       " 'meletakkan': 640130,\n",
       " 'salad': 767400,\n",
       " 'dressing': 152342,\n",
       " 'celery': 108326,\n",
       " 'kebimbangan': 581287,\n",
       " 'tersandung': 831468,\n",
       " 'alicecooper': 37388,\n",
       " 'perempuan': 720327,\n",
       " 'pendek': 716090,\n",
       " 'kandang': 576314,\n",
       " 'ditahan': 146442,\n",
       " 'memanggil': 640979,\n",
       " 'janji': 560151,\n",
       " 'sepenuhnya': 782982,\n",
       " 'salah': 767426,\n",
       " 'ucapan': 850064,\n",
       " 'menemuinya': 644711,\n",
       " 'memaafkan': 640740,\n",
       " 'bung': 101137,\n",
       " 'bah': 67095,\n",
       " 'stoopid': 807370,\n",
       " 'teruk': 832160,\n",
       " 'penuh': 718548,\n",
       " 'batuk': 72764,\n",
       " '9': 17697,\n",
       " 'meninggalkan': 647869,\n",
       " 'peperiksaan': 719338,\n",
       " 'day2': 128930,\n",
       " 'berasa': 78525,\n",
       " 'livingstone': 618114,\n",
       " 'letak': 614613,\n",
       " 'ikr': 544463,\n",
       " 'gambar': 186140,\n",
       " 'merindui': 651144,\n",
       " 'darah': 127674,\n",
       " 'argentina': 52713,\n",
       " 'siap': 791637,\n",
       " 'brazil': 96292,\n",
       " 'isnt': 553821,\n",
       " 'dont': 150596,\n",
       " 'goooo': 195076,\n",
       " 'senarai': 781383,\n",
       " 'rindukan': 756394,\n",
       " 'hey': 212107,\n",
       " 'tom': 840470,\n",
       " 'bolehkah': 93494,\n",
       " 'nazi': 676507,\n",
       " 'jut': 571932,\n",
       " 'berfikir': 79796,\n",
       " 'tamat': 821411,\n",
       " 'pengajian': 716554,\n",
       " 'memerlukan': 642257,\n",
       " 'sporky': 804219,\n",
       " 'dipisahkan': 143794,\n",
       " 'anakanak': 44431,\n",
       " 'menyayat': 649417,\n",
       " 'sampe': 768956,\n",
       " 'ga': 184564,\n",
       " 'sempat': 780801,\n",
       " 'pamit': 707601,\n",
       " 'anak2nya': 44416,\n",
       " 'balita': 68766,\n",
       " 'smua': 799151,\n",
       " 'berbaring': 78814,\n",
       " 'kulit': 603962,\n",
       " 'merangkak': 650468,\n",
       " 'pemikiran': 715017,\n",
       " 'pemanasan': 714203,\n",
       " 'kebun': 581488,\n",
       " 'miskin': 656988,\n",
       " 'frickin': 182005,\n",
       " 'fros': 182401,\n",
       " 'ryan': 764207,\n",
       " 'memang': 640967,\n",
       " 'akademi': 33152,\n",
       " 'chillin': 112383,\n",
       " 'bercanda': 79173,\n",
       " 'bosan': 94788,\n",
       " 'vaat': 856541,\n",
       " 'tonite': 840923,\n",
       " 'ab': 19819,\n",
       " 'tak': 820010,\n",
       " 'pagaar': 706021,\n",
       " 'nahin': 672111,\n",
       " 'mila': 654436,\n",
       " 'hai': 201621,\n",
       " 'tweeted': 847812,\n",
       " 'apis': 50593,\n",
       " 'menyegarkan': 649502,\n",
       " 'iphone': 551602,\n",
       " 'blah': 90398,\n",
       " 'token': 839978,\n",
       " 'refs': 751394,\n",
       " 'tokin': 840005,\n",
       " 'nampak': 673632,\n",
       " 'membeli': 641501,\n",
       " 'kekacauan': 583587,\n",
       " 'dahulu': 125570,\n",
       " 'sentuhan': 782333,\n",
       " 'berfungsi': 79825,\n",
       " 'imac': 545427,\n",
       " 'saluran': 768185,\n",
       " 'melbourne': 639852,\n",
       " 'peminat': 715146,\n",
       " 'maksudnya': 628037,\n",
       " 'blackberry': 90207,\n",
       " 'segala': 776692,\n",
       " 'maklumatnya': 627777,\n",
       " 'disandarkan': 144973,\n",
       " 'laptop': 610420,\n",
       " 'kunci': 604308,\n",
       " 'kemunculan': 585914,\n",
       " 'bergelut': 79970,\n",
       " 'terhad': 829486,\n",
       " 'idea': 542004,\n",
       " 'memperbaharui': 642615,\n",
       " 'halloween': 203084,\n",
       " 'uni': 853198,\n",
       " 'membiarkan': 641714,\n",
       " 'menghancurkan': 646571,\n",
       " 'kaca': 572854,\n",
       " 'gagal': 185259,\n",
       " 'jangan': 559922,\n",
       " 'mainmain': 626634,\n",
       " 'bersamasama': 83065,\n",
       " 'waktu': 862230,\n",
       " 'terjaga': 829893,\n",
       " 'pelatih': 713669,\n",
       " 'ppppoke': 732146,\n",
       " 'mukanya': 666387,\n",
       " 'bersinar': 83539,\n",
       " 'cahaya': 103782,\n",
       " 'katil': 578983,\n",
       " 'selesa': 779312,\n",
       " 'adakah': 22496,\n",
       " 'sebentar': 775299,\n",
       " 'eeep': 157505,\n",
       " 'izinkan': 556466,\n",
       " 'kebencian': 581111,\n",
       " 'tua': 844990,\n",
       " 'tertua': 832065,\n",
       " '17': 5503,\n",
       " 'gm': 193466,\n",
       " 'tweets': 847963,\n",
       " 'tweet': 847755,\n",
       " 'sitll': 795659,\n",
       " 'menghembuskan': 646669,\n",
       " 'nafas': 671820,\n",
       " 'album': 36046,\n",
       " 'mylife': 669562,\n",
       " 'rooled': 760451,\n",
       " '1': 2389,\n",
       " 'dang': 126677,\n",
       " 'songcry': 801595,\n",
       " 'panjang': 708506,\n",
       " 'janjikan': 560170,\n",
       " 'xoxo': 873812,\n",
       " 'kelaparan': 584245,\n",
       " 'masak': 633312,\n",
       " 'perut': 722884,\n",
       " 'ive': 555692,\n",
       " 'hmpf': 215394,\n",
       " 'barubaru': 71797,\n",
       " 'mendapati': 644165,\n",
       " 'tentu': 827893,\n",
       " 'berkelip': 81224,\n",
       " 'melawat': 639702,\n",
       " 'tn': 839280,\n",
       " 'dekat': 131340,\n",
       " 'atlanta': 58948,\n",
       " 'cukup': 122975,\n",
       " 'sana': 769217,\n",
       " 'ditambah': 146521,\n",
       " 'selsema': 779740,\n",
       " 'babi': 65175,\n",
       " 'grrr': 197110,\n",
       " 'atl': 58947,\n",
       " 'dijuluki': 139741,\n",
       " 'sheldon': 788763,\n",
       " 'sahabat': 766179,\n",
       " 'persamaan': 722058,\n",
       " 'watak': 864215,\n",
       " 'kuliah': 603885,\n",
       " 'pastikan': 710769,\n",
       " 'the': 833817,\n",
       " 'maraons': 631737,\n",
       " 'ref': 751208,\n",
       " 'kecerdasan': 581740,\n",
       " 'menghiraukan': 646776,\n",
       " 'dahsyat': 125546,\n",
       " 'tamatkan': 821415,\n",
       " 'yo': 879380,\n",
       " 'dude': 153548,\n",
       " 'carrie': 106630,\n",
       " 'bagaimanakah': 66659,\n",
       " 'brantanamo': 96166,\n",
       " 'sayangku': 772643,\n",
       " 'dengar': 132373,\n",
       " 'dm': 148681,\n",
       " 'lula': 622394,\n",
       " 'biara': 86723,\n",
       " 'wkend': 869104,\n",
       " 'rapat': 747966,\n",
       " 'kuku': 603778,\n",
       " 'naungan': 676087,\n",
       " 'sampai': 768894,\n",
       " 'jalanjalan': 559181,\n",
       " 'licin': 615787,\n",
       " 'sims': 793775,\n",
       " 'scrum': 774288,\n",
       " '14': 4462,\n",
       " 'cuti': 123817,\n",
       " 'ketidaksabaran': 590157,\n",
       " 'sg': 786159,\n",
       " 'mengesahkan': 646012,\n",
       " 'ke8': 580518,\n",
       " 'h1n1': 199565,\n",
       " 'selesema': 779366,\n",
       " 'harfiah': 206132,\n",
       " 'saat': 764920,\n",
       " 'terlambat': 830435,\n",
       " 'terjejas': 829966,\n",
       " '91n': 17870,\n",
       " 'tajam': 819950,\n",
       " 'teruja': 832156,\n",
       " 'selalu': 778743,\n",
       " 'pusingan': 738468,\n",
       " 'seterusnya': 785027,\n",
       " 'p': 705071,\n",
       " '2mow': 10305,\n",
       " 'feelin': 174873,\n",
       " 'like': 616276,\n",
       " 'poo': 730675,\n",
       " 'penyakit': 718716,\n",
       " 'minit': 655769,\n",
       " 'angsa': 46677,\n",
       " 'bebas': 74595,\n",
       " 'isobel': 553837,\n",
       " 'ditinggalkan': 147113,\n",
       " 'gah': 185342,\n",
       " 'dvd': 154982,\n",
       " 'berhenti': 80277,\n",
       " 'disewa': 145576,\n",
       " 'dimainkan': 141531,\n",
       " 'betul': 85504,\n",
       " 'gembira': 189157,\n",
       " 'kesepian': 589156,\n",
       " 'geometri': 189962,\n",
       " 'mommys': 661069,\n",
       " 'gemuk': 189389,\n",
       " 'maggi': 625646,\n",
       " 'berair': 78190,\n",
       " 'lecet': 612433,\n",
       " 'venezuela': 857853,\n",
       " 'ready': 749937,\n",
       " 'jk': 566852,\n",
       " 'mcfly': 637313,\n",
       " 'mango': 630492,\n",
       " 'mike': 654203,\n",
       " 'alexandria': 36540,\n",
       " '69': 15718,\n",
       " 'membenarkan': 641528,\n",
       " 'geladak': 188774,\n",
       " 'bermakna': 82030,\n",
       " 'hayfeverish': 208350,\n",
       " 'terbakar': 828516,\n",
       " 'seekor': 776515,\n",
       " 'burung': 101919,\n",
       " 'merpati': 651352,\n",
       " 'kotor': 600663,\n",
       " 'jy': 572251,\n",
       " 'twiiter': 848184,\n",
       " 'jong': 568786,\n",
       " 'kop': 599584,\n",
       " 'pelihat': 713846,\n",
       " 'hampir': 203662,\n",
       " 'berjaga': 80711,\n",
       " 'bernie': 82353,\n",
       " 'kesalahan': 588728,\n",
       " 'ejaan': 158831,\n",
       " 'leannas': 612101,\n",
       " 'mau': 635573,\n",
       " 'minum': 656130,\n",
       " 'makalah': 627117,\n",
       " 'potensi': 731619,\n",
       " 'biarkan': 86747,\n",
       " 'kimia': 593472,\n",
       " 'qld': 740450,\n",
       " 'terbenam': 828675,\n",
       " 'mesej': 651578,\n",
       " 'peeps': 712809,\n",
       " 'november': 691081,\n",
       " 'internet': 550836,\n",
       " 'dihidupkan': 139076,\n",
       " 'stesen': 806802,\n",
       " 'metro': 652026,\n",
       " 'mengelakkan': 645722,\n",
       " 'sesak': 784358,\n",
       " 'hijau': 213551,\n",
       " 'lelah': 613232,\n",
       " 'fielllld': 176186,\n",
       " 'suasana': 808714,\n",
       " 'ahli': 28450,\n",
       " 'penglihatan': 717563,\n",
       " 'pakej': 706810,\n",
       " 'jawa': 561262,\n",
       " 'kehidupan': 582968,\n",
       " 'rumit': 763309,\n",
       " 'berusaha': 84511,\n",
       " 'laman': 609053,\n",
       " 'sesuatu': 784668,\n",
       " 'scrapballerina': 774125,\n",
       " 'info': 548234,\n",
       " 'pinabile': 725954,\n",
       " 'lng': 618695,\n",
       " 'sa': 764791,\n",
       " 'akin': 33795,\n",
       " 'yan': 876074,\n",
       " 'dati': 128496,\n",
       " 'eh': 158098,\n",
       " 'sakin': 767095,\n",
       " 'nagrenew': 672033,\n",
       " 'lala': 608661,\n",
       " 'kelabu': 584016,\n",
       " 'join': 568242,\n",
       " 'tweetknot': 847895,\n",
       " 'imma': 546092,\n",
       " 'mengatakan': 645490,\n",
       " 'kenal': 586012,\n",
       " 'pelik': 713849,\n",
       " 'coklat': 118008,\n",
       " 'menyusut': 650122,\n",
       " 'menyalahkan': 649242,\n",
       " 'kemelesetan': 585522,\n",
       " 'ayam': 61968,\n",
       " 'mexican': 652233,\n",
       " 'hmm': 215265,\n",
       " 'bersendirian': 83324,\n",
       " 'sukacita': 810201,\n",
       " 'blackberrys': 90211,\n",
       " 'bateri': 72471,\n",
       " 'sob': 799984,\n",
       " 'kemas': 585292,\n",
       " 'fucking': 183137,\n",
       " 'ss': 805086,\n",
       " 'fail': 169522,\n",
       " 'souce': 802759,\n",
       " 'tertidur': 831994,\n",
       " 'api': 50495,\n",
       " 'tiub': 838404,\n",
       " 'alias': 37314,\n",
       " 'havent': 208106,\n",
       " 'pettin': 723597,\n",
       " 'scratchin': 774139,\n",
       " 'lari': 610566,\n",
       " 'cacat': 103510,\n",
       " 'berikan': 80412,\n",
       " 'beta': 85144,\n",
       " 'penguji': 717696,\n",
       " 'ahh': 28259,\n",
       " 'bilik': 87965,\n",
       " 'bola': 93340,\n",
       " 'sepak': 782723,\n",
       " 'asal': 55260,\n",
       " 'sesungguhnya': 784720,\n",
       " 'kambing': 575409,\n",
       " 'hak': 202384,\n",
       " 'berpakaian': 82480,\n",
       " 'kucing': 603169,\n",
       " 'smurf': 799183,\n",
       " 'usia': 855302,\n",
       " 'nafsu': 671920,\n",
       " 'pertandingan': 722544,\n",
       " 'populariti': 730960,\n",
       " 'undi': 852914,\n",
       " 'untuknya': 853921,\n",
       " 'sungguh': 811439,\n",
       " 'perasaan': 719752,\n",
       " 'disapu': 145028,\n",
       " 'bergaul': 79907,\n",
       " 'pintar': 726398,\n",
       " 'terkena': 830201,\n",
       " 'trak': 842359,\n",
       " 'abbigale': 20138,\n",
       " 'mari': 632078,\n",
       " 'lihat': 616203,\n",
       " 'lengannya': 613943,\n",
       " 'gadis': 185034,\n",
       " 'tweeting': 847880,\n",
       " 'm': 623968,\n",
       " 'com': 118386,\n",
       " 'bila': 87811,\n",
       " 'mengagumkan': 645030,\n",
       " 'malu': 629026,\n",
       " 'romantis': 760124,\n",
       " 'boohoo': 94170,\n",
       " 'oatmealnya': 697499,\n",
       " 'kekuatan': 583927,\n",
       " 'am': 40116,\n",
       " 'percutian': 720173,\n",
       " 'senang': 781302,\n",
       " 'talaga': 821090,\n",
       " 'beku': 76128,\n",
       " 'estet': 165387,\n",
       " 'ambattur': 40868,\n",
       " 'utterli': 855915,\n",
       " 'nampaknya': 673654,\n",
       " 'uh': 850747,\n",
       " 'pasport': 710631,\n",
       " 'obama': 697528,\n",
       " 'ibunya': 541380,\n",
       " 'pelacur': 713429,\n",
       " 'perbualan': 719976,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000     hari ini. entah bagaimana ia tidak kelihatan sangat penting. hidup agak sucks sekarang. saya perlukan beberapa rancangan.                                                           \n",
      "160001     marah. saya sangat tidak suka ineptitude dan cenderung berpandangan yang keras kepala. tuhan yang sayang, tolong saya daripada membunuh sesiapa sahaja di tempat kerja              \n",
      "160002     ia figgers. satu hari minggu ini saya boleh pulang pada pukul 6 petang, dan lalu lintas menghisap sekarang terperangkap di pejabat yang merasa bersalah kerana tidak bekerja. lt    \n",
      "160003     pagi esok. . . matahari, tidak hujan setakat ini. . . sakit kepala alergi lebih baik, tetapi tidak benar-benar hilang. . . akan berehat hari ini. . . ingin saya boleh duduk di luar\n",
      "160004     8. 5 jam lagi. . . .                                                                                                                                                                \n",
      "                   ...                                                                                                                                                                         \n",
      "1554057    @veranvt Jadi ukhti dong, entar banyak banget yang suka sama lu, sumpah kalo pake hijab mah                                                                                         \n",
      "1554058    @spielpIatz hy sayang kenalan yu                                                                                                                                                    \n",
      "1554059    /wal aku cantik lah kelonya ku cantik?                                                                                                                                              \n",
      "1554060    Wtb slogan! Stray kids hyunjin atau IN drop dong\\r\\nKalo cocok mau aku take                                                                                                         \n",
      "1554062    Orang yg hebat boleh menghasilkan sebuah karya agung. Tapi Guru yg hebat mampu melahirkan ribuan orang yang lebih h https://t.co/6Yth9Lrrsx                                         \n",
      "Name: tweet, Length: 1185758, dtype: object\n",
      "\n",
      "\n",
      "  (0, 0)\t502\n",
      "  (0, 1)\t81\n",
      "  (0, 2)\t221\n",
      "  (0, 3)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 31)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 56)\t3\n",
      "  (0, 57)\t13\n",
      "  (0, 61)\t1\n",
      "  (0, 75)\t5\n",
      "  (0, 76)\t2\n",
      "  (0, 83)\t1\n",
      "  (0, 88)\t3\n",
      "  (0, 89)\t1\n",
      "  (0, 94)\t1\n",
      "  (0, 95)\t3\n",
      "  (0, 99)\t1\n",
      "  :\t:\n",
      "  (0, 887308)\t1\n",
      "  (0, 887309)\t1\n",
      "  (0, 887310)\t1\n",
      "  (0, 887311)\t1\n",
      "  (0, 887312)\t1\n",
      "  (0, 887313)\t1\n",
      "  (0, 887314)\t3\n",
      "  (0, 887316)\t2\n",
      "  (0, 887317)\t1\n",
      "  (0, 887318)\t1\n",
      "  (0, 887319)\t2\n",
      "  (0, 887320)\t5\n",
      "  (0, 887321)\t1\n",
      "  (0, 887322)\t1\n",
      "  (0, 887323)\t1\n",
      "  (0, 887324)\t1\n",
      "  (0, 887325)\t1\n",
      "  (0, 887326)\t1\n",
      "  (0, 887327)\t1\n",
      "  (0, 887328)\t1\n",
      "  (0, 887330)\t1\n",
      "  (0, 887331)\t1\n",
      "  (0, 887332)\t1\n",
      "  (0, 887334)\t1\n",
      "  (0, 887335)\t5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of vectorized text\n",
    "sample_tweet = data['tweet']\n",
    "print(sample_tweet)\n",
    "print('\\n')\n",
    "# vector representation\n",
    "bow_sample = bow_transformer.transform([sample_tweet])\n",
    "print(bow_sample)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (1185758, 887336)\n",
      "Amount of Non-Zero occurences:  10482994\n"
     ]
    }
   ],
   "source": [
    "# transform the entire DataFrame of messages\n",
    "messages_bow = bow_transformer.transform(data['tweet'])\n",
    "\n",
    "# check out the bag-of-words counts for the entire corpus as a large sparse matrix\n",
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Term Frequency, Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 887335)\t5.449045700694676e-05\n",
      "  (0, 887334)\t1.0898091401389353e-05\n",
      "  (0, 887332)\t1.0898091401389353e-05\n",
      "  (0, 887331)\t1.0898091401389353e-05\n",
      "  (0, 887330)\t1.0898091401389353e-05\n",
      "  (0, 887328)\t1.0898091401389353e-05\n",
      "  (0, 887327)\t1.0898091401389353e-05\n",
      "  (0, 887326)\t1.0898091401389353e-05\n",
      "  (0, 887325)\t1.0898091401389353e-05\n",
      "  (0, 887324)\t1.0898091401389353e-05\n",
      "  (0, 887323)\t1.0898091401389353e-05\n",
      "  (0, 887322)\t1.0898091401389353e-05\n",
      "  (0, 887321)\t1.0898091401389353e-05\n",
      "  (0, 887320)\t4.971435418459469e-05\n",
      "  (0, 887319)\t2.1177855762180886e-05\n",
      "  (0, 887318)\t1.0898091401389353e-05\n",
      "  (0, 887317)\t1.0898091401389353e-05\n",
      "  (0, 887316)\t2.1177855762180886e-05\n",
      "  (0, 887314)\t3.110871861769321e-05\n",
      "  (0, 887313)\t1.0898091401389353e-05\n",
      "  (0, 887312)\t1.0898091401389353e-05\n",
      "  (0, 887311)\t1.0898091401389353e-05\n",
      "  (0, 887310)\t1.0898091401389353e-05\n",
      "  (0, 887309)\t1.0898091401389353e-05\n",
      "  (0, 887308)\t1.0898091401389353e-05\n",
      "  :\t:\n",
      "  (0, 99)\t1.0898091401389353e-05\n",
      "  (0, 95)\t2.925373749589976e-05\n",
      "  (0, 94)\t1.0588927881090443e-05\n",
      "  (0, 89)\t1.0898091401389353e-05\n",
      "  (0, 88)\t2.925373749589976e-05\n",
      "  (0, 83)\t1.0369572872564404e-05\n",
      "  (0, 76)\t2.1177855762180886e-05\n",
      "  (0, 75)\t4.920527171869727e-05\n",
      "  (0, 61)\t1.0588927881090443e-05\n",
      "  (0, 57)\t0.00011704383983000277\n",
      "  (0, 56)\t3.018122805679648e-05\n",
      "  (0, 55)\t1.0898091401389353e-05\n",
      "  (0, 53)\t1.0898091401389353e-05\n",
      "  (0, 52)\t1.0898091401389353e-05\n",
      "  (0, 49)\t1.0898091401389353e-05\n",
      "  (0, 48)\t1.0898091401389353e-05\n",
      "  (0, 42)\t1.0369572872564404e-05\n",
      "  (0, 34)\t1.0369572872564404e-05\n",
      "  (0, 31)\t1.0898091401389353e-05\n",
      "  (0, 24)\t2.9828612510756815e-05\n",
      "  (0, 14)\t1.0588927881090443e-05\n",
      "  (0, 3)\t8.541199162911708e-06\n",
      "  (0, 2)\t0.0015888967839977352\n",
      "  (0, 1)\t0.0006194747241926049\n",
      "  (0, 0)\t0.0031671018748699094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf_sample = tfidf_transformer.transform(bow_sample)\n",
    "print(tfidf_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1185758, 887336)\n"
     ]
    }
   ],
   "source": [
    "# to transform the entire bag-of-words corpus\n",
    "tweet_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(tweet_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries for dataset preparation, feature engineering, model training\n",
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import (RandomUnderSampler, \n",
    "                                    NearMiss, \n",
    "                                    InstanceHardnessThreshold,\n",
    "                                    CondensedNearestNeighbour,\n",
    "                                    EditedNearestNeighbours,\n",
    "                                    RepeatedEditedNearestNeighbours,\n",
    "                                    AllKNN,\n",
    "                                    NeighbourhoodCleaningRule,\n",
    "                                    OneSidedSelection,\n",
    "                                    TomekLinks)\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and test datasets \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data['tweet'],data['label'], test_size=0.2)\n",
    "#label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
    "tfidf_vect.fit(data['tweet'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
    "xtest_tfidf =  tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the f1 Score\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.f1_score(y_test,predictions,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Over Sampling\n",
    "ros = RandomOverSampler(random_state=777)\n",
    "ros_xtrain_tfidf, ros_ytrain = ros.fit_sample(xtrain_tfidf, y_train)\n",
    "accuracyROS = train_model(LinearSVC(class_weight='balanced', penalty='l2', loss='hinge', max_iter=1000000),ros_xtrain_tfidf, ros_ytrain, xtest_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ROS:  0.8532040628735464\n"
     ]
    }
   ],
   "source": [
    "print (\"SVM ROS: \", accuracyROS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_sentiment_ROS.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model to current working directory\n",
    "joblib.dump(accuracyROS, \"twitter_sentiment_ROS.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE()\n",
    "model = Pipeline([\n",
    "    ('sampling', SMOTE()),\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', LinearSVC(class_weight='balanced', penalty='l2', loss='hinge', max_iter=100000))\n",
    "])\n",
    "\n",
    "sm_xtrain_tfidf, sm_ytrain = sm.fit_sample(xtrain_tfidf, y_train)\n",
    "accuracySMOTE = train_model(model, sm_xtrain_tfidf, sm_ytrain, xtest_tfidf)\n",
    "print (\"SVC SMOTE, WordLevel TFIDF: \", accuracySMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', LinearSVC(class_weight='balanced', penalty='l2', loss='hinge', random_state=777, max_iter=100000))\n",
    "])\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)\n",
    "\n",
    "print('accuracy score: ',accuracy_score(y_test, y_preds))\n",
    "print('\\n')\n",
    "print('confusion matrix: \\n',confusion_matrix(y_test,y_preds))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 40.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 125.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 138.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: 0.533935 using {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "\n",
      "\n",
      "Mean: 0.524345 Stdev:(0.005149) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.526015 Stdev:(0.008175) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.519057 Stdev:(0.000766) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518440 Stdev:(0.000366) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.528067 Stdev:(0.008229) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.529299 Stdev:(0.008719) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.520888 Stdev:(0.001753) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.525403 Stdev:(0.008721) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.530729 Stdev:(0.004090) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.533935 Stdev:(0.009624) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.527506 Stdev:(0.008674) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.526489 Stdev:(0.006415) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.526593 Stdev:(0.009120) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.523667 Stdev:(0.001988) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.521017 Stdev:(0.004891) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.519978 Stdev:(0.001614) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.528325 Stdev:(0.006676) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.530291 Stdev:(0.008666) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.523120 Stdev:(0.006050) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.522669 Stdev:(0.006511) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.531000 Stdev:(0.005647) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.533047 Stdev:(0.008188) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.527100 Stdev:(0.007933) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.527835 Stdev:(0.007532) with: {'bow__ngram_range': (1, 1), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.521425 Stdev:(0.002856) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.521798 Stdev:(0.007051) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.518096 Stdev:(0.000092) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518002 Stdev:(0.000074) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.523277 Stdev:(0.006615) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.521228 Stdev:(0.002104) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.518547 Stdev:(0.000634) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.519264 Stdev:(0.001746) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.522482 Stdev:(0.001834) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.522818 Stdev:(0.003137) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.519174 Stdev:(0.001673) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.521937 Stdev:(0.007002) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'gini', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.519103 Stdev:(0.000695) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.519961 Stdev:(0.002357) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.518233 Stdev:(0.000476) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518445 Stdev:(0.000709) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.520105 Stdev:(0.001031) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.523009 Stdev:(0.004322) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.520180 Stdev:(0.002661) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518358 Stdev:(0.000392) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 5, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n",
      "Mean: 0.521724 Stdev:(0.001755) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': True}\n",
      "Mean: 0.521554 Stdev:(0.001940) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 20, 'tfidf__use_idf': False}\n",
      "Mean: 0.519791 Stdev:(0.001570) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': True}\n",
      "Mean: 0.518961 Stdev:(0.000818) with: {'bow__ngram_range': (1, 2), 'clf__criterion': 'entropy', 'clf__max_depth': 6, 'clf__max_features': 'auto', 'clf__n_estimators': 50, 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', RandomForestClassifier()),  \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__n_estimators': [20, 50],\n",
    "                'clf__max_features': ['auto'],\n",
    "                'clf__max_depth' : [4,5,6],\n",
    "                'clf__criterion' :['gini', 'entropy'],\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed: 65.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: 0.836509 using {'bow__ngram_range': (1, 2), 'clf__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "\n",
      "\n",
      "Mean: 0.803009 Stdev:(0.001064) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.01, 'tfidf__use_idf': True}\n",
      "Mean: 0.818192 Stdev:(0.001022) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "Mean: 0.799113 Stdev:(0.000957) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.001, 'tfidf__use_idf': True}\n",
      "Mean: 0.812924 Stdev:(0.001147) with: {'bow__ngram_range': (1, 1), 'clf__alpha': 0.001, 'tfidf__use_idf': False}\n",
      "Mean: 0.823180 Stdev:(0.001459) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.01, 'tfidf__use_idf': True}\n",
      "Mean: 0.836509 Stdev:(0.001448) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "Mean: 0.810987 Stdev:(0.001503) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.001, 'tfidf__use_idf': True}\n",
      "Mean: 0.825470 Stdev:(0.001383) with: {'bow__ngram_range': (1, 2), 'clf__alpha': 0.001, 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', MultinomialNB()),  \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__alpha': (1e-2, 1e-3),\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_sentiment_NB.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save best model to current working directory\n",
    "joblib.dump(grid, \"twitter_sentiment_NB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.6min\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', SGDClassifier(class_weight='balanced')),  \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__loss' : [\"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "                'clf__alpha': (1e-3, 1e-4),\n",
    "                'clf__penalty' : [\"l2\", \"none\"],\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed: 130.0min\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced', max_iter=100000))) \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'bow__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__estimator__loss' : [\"hinge\", \"squared_hinge\"],\n",
    "                'clf__estimator__penalty' : [\"l2\", \"none\"], \n",
    "                'clf__estimator__tol' : [1e-4, 1e-5]\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, verbose=1, n_jobs= 8)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model to current working directory\n",
    "joblib.dump(grid, \"twitter_sentiment_LSVM_balanced_bm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSVC = joblib.load(\"twitter_sentiment_LSVM_balanced_bm.pkl\" )\n",
    "\n",
    "# get predictions from best model above\n",
    "y_preds = model_LSVC.predict(X_test)\n",
    "\n",
    "print('accuracy score: ',accuracy_score(y_test, y_preds))\n",
    "print('\\n')\n",
    "print('confusion matrix: \\n',confusion_matrix(y_test,y_preds))\n",
    "print('\\n')\n",
    "print(metrics.classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(\n",
    "    y_test, \n",
    "    y_preds,\n",
    "    figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class ClfSwitcher(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        estimator = SGDClassifier(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "        :param estimator: sklearn object - The classifier\n",
    "        \"\"\" \n",
    "\n",
    "        self.estimator = estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    }
   ],
   "source": [
    "# Run Train Data Through Pipeline analyzer=text_process\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)\n",
    "\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            lowercase=True)),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', ClfSwitcher()),  \n",
    "])\n",
    "\n",
    "# this is where we define the values for GridSearchCV to iterate over\n",
    "parameters = [\n",
    "            {\n",
    "                'clf__estimator': [SGDClassifier()],\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__estimator__loss' : [\"squared_hinge\", \"modified_huber\"],\n",
    "                'clf__estimator__alpha' : [0.001, 0.01, 0.1],\n",
    "                'clf__estimator__penalty' : [\"l2\"],\n",
    "            },\n",
    "            {\n",
    "                'clf__estimator': [MultinomialNB()],\n",
    "                'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__estimator__alpha': (1e-2, 1e-3),\n",
    "            },\n",
    "            ]\n",
    "\n",
    "# do 10-fold cross validation for each of the 6 possible combinations of the above params\n",
    "grid = GridSearchCV(pipeline, cv=5, param_grid=parameters, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model to current working directory\n",
    "joblib.dump(grid, \"twitter_sentiment_ensemble_NB_SGD.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanan\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator CountVectorizer from version 0.22.2.post1 when using version 0.23.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\Users\\nanan\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 0.23.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\Users\\nanan\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator LinearSVC from version 0.22.2.post1 when using version 0.23.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\Users\\nanan\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator OneVsRestClassifier from version 0.22.2.post1 when using version 0.23.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\Users\\nanan\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator LabelBinarizer from version 0.22.2.post1 when using version 0.23.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\Users\\nanan\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator GridSearchCV from version 0.22.2.post1 when using version 0.23.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9481164303793537\n",
      "\n",
      "\n",
      "confusion matrix: \n",
      " [[119360   7508    205]\n",
      " [  4694 122914    350]\n",
      " [   214    394   1957]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95    127073\n",
      "           2       0.94      0.96      0.95    127958\n",
      "           4       0.78      0.76      0.77      2565\n",
      "\n",
      "    accuracy                           0.95    257596\n",
      "   macro avg       0.89      0.89      0.89    257596\n",
      "weighted avg       0.95      0.95      0.95    257596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_LSVC = joblib.load(\"twitter_sentiment_LSVM_balanced_final.pkl\" )\n",
    "\n",
    "# get predictions from best model above\n",
    "y_preds = model_LSVC.predict(X_test)\n",
    "\n",
    "print('accuracy score: ',accuracy_score(y_test, y_preds))\n",
    "print('\\n')\n",
    "print('confusion matrix: \\n',confusion_matrix(y_test,y_preds))\n",
    "print('\\n')\n",
    "print(metrics.classification_report(y_test, y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
